"""MCP Tools for media processing"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import subprocess
import os
from pathlib import Path
from utils.media_utils import validate_video_file, parse_time, format_time, ensure_output_dir
from utils.logger import get_logger

logger = get_logger(__name__)

# Image processing imports
try:
    from PIL import Image, ImageOps
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None
    ImageOps = None

# YOLO imports for object detection
try:
    from ultralytics import YOLO
    import numpy as np
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    YOLO = None
    np = None


class MediaTool(ABC):
    """Base class for media processing tools"""
    
    def __init__(self, ffmpeg_path: str = "ffmpeg", ffprobe_path: str = "ffprobe"):
        self.ffmpeg_path = ffmpeg_path
        self.ffprobe_path = ffprobe_path
    
    @abstractmethod
    def execute(self, **kwargs) -> Dict[str, Any]:
        """Execute the tool"""
        pass
    
    def _run_ffmpeg(self, cmd: list, timeout: int = 300) -> Dict[str, Any]:
        """Run FFmpeg command"""
        try:
            result = subprocess.run(
                [self.ffmpeg_path] + cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                check=True,
                encoding='utf-8',
                errors='ignore'  # å¿½ç•¥ç¼–ç é”™è¯¯
            )
            return {"success": True, "output": result.stdout}
        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Command timeout"}
        except subprocess.CalledProcessError as e:
            return {"success": False, "error": e.stderr}
        except Exception as e:
            return {"success": False, "error": str(e)}


class ClipTool(MediaTool):
    """Tool for video clipping"""
    
    def execute(
        self,
        input_path: str,
        start_time: str,
        end_time: str,
        output_path: str,
        **kwargs
    ) -> Dict[str, Any]:
        """Clip video from start_time to end_time"""
        if not validate_video_file(input_path):
            return {"success": False, "error": "Invalid video file"}
        
        output_path = str(ensure_output_dir(output_path))
        
        start_seconds = parse_time(start_time)
        end_seconds = parse_time(end_time)
        duration = end_seconds - start_seconds
        
        cmd = [
            "-i", input_path,
            "-ss", str(start_seconds),
            "-t", str(duration),
            "-c", "copy",  # Copy codec for faster processing
            "-avoid_negative_ts", "make_zero",
            output_path,
            "-y"  # Overwrite output file
        ]
        
        return self._run_ffmpeg(cmd)


class ConcatTool(MediaTool):
    """Tool for concatenating videos"""
    
    def execute(
        self,
        video_paths: list,
        output_path: str,
        **kwargs
    ) -> Dict[str, Any]:
        """Concatenate multiple videos"""
        # Validate all input files
        for path in video_paths:
            if not validate_video_file(path):
                return {"success": False, "error": f"Invalid video file: {path}"}
        
        output_path = str(ensure_output_dir(output_path))
        
        # Create concat file list
        concat_file = output_path + ".concat.txt"
        with open(concat_file, 'w', encoding='utf-8') as f:
            for path in video_paths:
                f.write(f"file '{os.path.abspath(path)}'\n")
        
        try:
            cmd = [
                "-f", "concat",
                "-safe", "0",
                "-i", concat_file,
                "-c", "copy",
                output_path,
                "-y"
            ]
            
            result = self._run_ffmpeg(cmd)
            
            # Clean up concat file
            if os.path.exists(concat_file):
                os.remove(concat_file)
            
            return result
        except Exception as e:
            # Clean up concat file on error
            if os.path.exists(concat_file):
                os.remove(concat_file)
            return {"success": False, "error": str(e)}


class SubtitleTool(MediaTool):
    """Tool for subtitle generation using Whisper"""
    
    def __init__(self, ffmpeg_path: str = "ffmpeg", ffprobe_path: str = "ffprobe"):
        super().__init__(ffmpeg_path, ffprobe_path)
        self._whisper_model = None
        self._llm_manager = None
    
    def _get_whisper_model(self, model_name: str = "base"):
        """Lazy load Whisper model"""
        if self._whisper_model is None:
            try:
                import whisper
                import torch
                logger.info(f"Loading Whisper model: {model_name}")
                
                
                # Whisper åœ¨ GPU ä¸Šè¿è¡Œ
                device = "cuda" if torch.cuda.is_available() else "cpu"
                logger.info(f"Whisper using {device}")
                
                self._whisper_model = whisper.load_model(model_name, device=device)
                logger.info(f"Whisper model loaded successfully on {device}")
            except Exception as e:
                logger.error(f"Failed to load Whisper model: {e}")
                raise
        return self._whisper_model
    
    def _get_llm_manager(self):
        """Lazy load LLM manager for subtitle correction"""
        if self._llm_manager is None:
            try:
                import sys
                from pathlib import Path
                # Ensure llm module can be imported
                parent_dir = Path(__file__).parent.parent
                if str(parent_dir) not in sys.path:
                    sys.path.insert(0, str(parent_dir))
                
                from llm import LLMManager
                self._llm_manager = LLMManager()
                logger.info("LLM Manager initialized for subtitle correction")
            except Exception as e:
                logger.warning(f"Failed to initialize LLM Manager: {e}")
                self._llm_manager = None
        return self._llm_manager
    
    def _extract_technical_terms(self, video_path: str, initial_text: str) -> dict:
        """è‡ªåŠ¨ä»Žè§†é¢‘æ–‡ä»¶åå’Œåˆæ­¥å­—å¹•ä¸­æå–ä¸“ä¸šæœ¯è¯­"""
        llm_manager = self._get_llm_manager()
        if not llm_manager:
            logger.warning("LLM not available, cannot extract technical terms")
            return {}
        
        provider = llm_manager.get_provider(task_type="chinese_processing")
        if not provider:
            provider = llm_manager.get_provider()
        
        if not provider:
            return {}
        
        try:
            # ä»Žæ–‡ä»¶åæå–ä¿¡æ¯
            import os
            filename = os.path.basename(video_path)
            
            # å–å‰ 500 å­—ä½œä¸ºæ ·æœ¬ï¼ˆé¿å…å¤ªé•¿ï¼‰
            sample_text = initial_text[:500] if len(initial_text) > 500 else initial_text
            
            prompt = f"""è¯·ä»Žä»¥ä¸‹å­—å¹•æ–‡æœ¬ä¸­æå–æ‰€æœ‰ä¸“ä¸šæœ¯è¯­ã€å“ç‰Œåç§°ã€æŠ€æœ¯å…³é”®è¯ã€‚
ç‰¹åˆ«æ³¨æ„ï¼šè‹±æ–‡æœ¯è¯­ã€ç¼©å†™ã€å“ç‰Œåç­‰å¿…é¡»ä¿æŒåŽŸæ ·ã€‚

è§†é¢‘æ–‡ä»¶åï¼š{filename}
å­—å¹•æ–‡æœ¬æ ·æœ¬ï¼ˆå‰500å­—ï¼‰ï¼š
{sample_text}

è¯·ä»¥ JSON æ ¼å¼è¿”å›žä¸“ä¸šæœ¯è¯­åˆ—è¡¨ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "æœ¯è¯­1": "æœ¯è¯­1",
  "æœ¯è¯­2": "æœ¯è¯­2",
  ...
}}

å¸¸è§ä¸“ä¸šæœ¯è¯­å‚è€ƒï¼ˆæå–æ—¶ä¸é™äºŽä»¥ä¸‹ç¤ºä¾‹ï¼‰ï¼š
- ç¼–ç¨‹è¯­è¨€ï¼šPython, Java, JavaScript, TypeScript, Go, Rust
- æ¡†æž¶å·¥å…·ï¼šFastAPI, Django, Flask, React, Vue, Docker, Kubernetes
- AI/MLï¼šRAG, GPT, LLM, Transformer, PyTorch, TensorFlow, CUDA
- æŠ€æœ¯æ¦‚å¿µï¼šAPI, REST, GraphQL, WebSocket, JSON, YAML
- å“ç‰Œäº§å“ï¼šDeepSeek, Qwen, Whisper, OpenAI, HuggingFace, Ollama

ç¤ºä¾‹è¾“å‡ºï¼š
{{
  "DeepSeek": "DeepSeek",
  "RAG": "RAG",
  "FastAPI": "FastAPI",
  "Python": "Python",
  "GPU": "GPU"
}}

åªè¿”å›ž JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜Žã€‚"""
            
            # ä½¿ç”¨åŒæ­¥ chat æ–¹æ³•ï¼ˆå†…éƒ¨ä¼šå¤„ç†äº‹ä»¶å¾ªçŽ¯ï¼‰
            try:
                response = provider.chat(prompt)
                logger.info(f"LLM æœ¯è¯­æå–å“åº”: {response[:200] if response else 'Empty'}...")
            except Exception as e:
                logger.error(f"LLM chat è°ƒç”¨å¤±è´¥: {str(e)}")
                return {}
            
            # æå– JSON
            import json
            import re
            
            if not response:
                logger.warning("LLM è¿”å›žç©ºå“åº”")
                return {}
            
            # æ‰“å°å®Œæ•´å“åº”ç”¨äºŽè°ƒè¯•
            logger.info(f"LLM å®Œæ•´å“åº”: {response}")
            
            # å…ˆå°è¯•åŽ»é™¤ä»£ç å—æ ‡è®°ï¼ˆ```json ... ```ï¼‰
            cleaned_response = response.strip()
            if cleaned_response.startswith('```'):
                # åŽ»é™¤å¼€å¤´çš„ ```json æˆ– ```
                lines = cleaned_response.split('\n')
                if lines[0].startswith('```'):
                    lines = lines[1:]  # åŽ»é™¤ç¬¬ä¸€è¡Œ
                if lines and lines[-1].strip() == '```':
                    lines = lines[:-1]  # åŽ»é™¤æœ€åŽä¸€è¡Œ
                cleaned_response = '\n'.join(lines).strip()
            
            # å°è¯•æå– JSON å¯¹è±¡
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response, re.DOTALL)
            if json_match:
                try:
                    tech_terms = json.loads(json_match.group())
                    logger.info(f"è‡ªåŠ¨æå–äº† {len(tech_terms)} ä¸ªä¸“ä¸šæœ¯è¯­: {list(tech_terms.keys())[:10]}")
                    return tech_terms
                except json.JSONDecodeError as e:
                    logger.error(f"JSON è§£æžå¤±è´¥: {e}, æå–çš„æ–‡æœ¬: {json_match.group()[:200]}")
                    return {}
            else:
                logger.warning(f"LLM å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆ JSONï¼Œå“åº”å‰ 500 å­—ç¬¦: {response[:500]}")
                return {}
                
        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æžå¤±è´¥: {str(e)}")
            return {}
        except Exception as e:
            logger.error(f"æå–ä¸“ä¸šæœ¯è¯­å¤±è´¥: {str(e)}", exc_info=True)
            return {}
    
    def _correct_subtitle_with_llm(self, segments: list, **kwargs) -> list:
        """Use LLM to correct subtitle text based on context"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹ LLM æ™ºèƒ½çº é”™æµç¨‹")
        logger.info("=" * 60)
        
        llm_manager = self._get_llm_manager()
        if not llm_manager:
            logger.warning("âŒ LLM Manager ä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™çº é”™")
            return self._correct_subtitle_with_rules(segments)
        
        # Get provider for Chinese processing
        provider = llm_manager.get_provider(task_type="chinese_processing")
        if not provider:
            # Try to get any available provider
            logger.warning("âš ï¸ æœªæ‰¾åˆ° chinese_processing providerï¼Œå°è¯•èŽ·å–é»˜è®¤ provider")
            provider = llm_manager.get_provider()
        
        if not provider:
            logger.warning("âŒ æ²¡æœ‰å¯ç”¨çš„ LLM providerï¼Œä½¿ç”¨è§„åˆ™çº é”™")
            return self._correct_subtitle_with_rules(segments)
        
        logger.info(f"âœ… LLM Provider å·²èŽ·å–: {provider.__class__.__name__}")
        
        try:
            # åˆ†æ‰¹å¤„ç†ï¼šæ¯æ¬¡å¤„ç†50æ¡å­—å¹•ï¼ˆé¿å…promptè¿‡é•¿ï¼‰
            batch_size = 50
            total_segments = len(segments)
            logger.info(f"ðŸ“Š æ€»å…± {total_segments} æ¡å­—å¹•ï¼Œåˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ {batch_size} æ¡ï¼‰")
            
            corrected_segments = []
            for batch_start in range(0, total_segments, batch_size):
                batch_end = min(batch_start + batch_size, total_segments)
                batch = segments[batch_start:batch_end]
                
                logger.info(f"ðŸ”„ å¤„ç†ç¬¬ {batch_start+1}-{batch_end} æ¡å­—å¹•...")
                
                # Combine batch subtitle text for context
                full_text = "\n".join([f"{i+1}. {seg['text'].strip()}" for i, seg in enumerate(batch)])
                
                # èŽ·å–ä¸“ä¸šè¯æ±‡è¯å…¸ï¼Œæ•´åˆè‡ªåŠ¨æå–çš„æœ¯è¯­
                tech_terms = kwargs.get('tech_terms', {})
                
                # åŸºç¡€æŠ€æœ¯è¯æ±‡ï¼ˆä¿åº•ï¼‰
                base_terms = ["FastAPI", "DeepSeek", "R1", "Pydantic", "Ollama", "GPU", "API", 
                             "Docker", "Python", "RAG", "Transformer", "CUDA", "PyTorch", "TensorFlow",
                             "HuggingFace", "OpenAI", "GPT", "LLM", "Qwen", "Whisper", "FFmpeg"]
                
                # å¸¸è§æ‹¼éŸ³è¯¯è¯†åˆ«æ˜ å°„ï¼ˆå¸®åŠ©LLMè¯†åˆ«ï¼‰
                pinyin_map = {
                    "FastAPI": ["past api", "fast api", "æ³•æ–¯ç‰¹api", "å¸•æ–¯ç‰¹api"],
                    "RAG": ["rg", "é˜¿æ ¼", "r g"],
                    "DeepSeek": ["dpsi", "è¿ªæ™®è¥¿å…‹", "deep seek"],
                    "API": ["a p i", "åŸƒçš®çˆ±", "æŽ¥å£"],
                    "Web": ["æœªå¤‡", "å¾®åš"],
                    "Docker": ["å¤šå…‹", "é“å…‹"],
                    "GPU": ["g p u", "è®¡çš®å‹"],
                    "LLM": ["l l m", "å¤§æ¨¡åž‹"],
                    "Transformer": ["ä¼ è¾“ä½›èŽ«", "transformer"]
                }
                
                # åˆå¹¶è‡ªåŠ¨æå–çš„æœ¯è¯­ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
                if tech_terms:
                    extracted_terms = list(tech_terms.keys())
                    # åˆå¹¶åŽ»é‡
                    all_terms = list(dict.fromkeys(extracted_terms + base_terms))
                    # åªæ˜¾ç¤ºå‰15ä¸ªæœ€é‡è¦çš„æœ¯è¯­
                    key_terms = all_terms[:15]
                    logger.info(f"ðŸ”¤ ä½¿ç”¨ {len(all_terms)} ä¸ªä¸“ä¸šæœ¯è¯­ï¼Œé‡ç‚¹çº æ­£å‰15ä¸ª: {', '.join(key_terms)}")
                else:
                    key_terms = base_terms[:15]
                    logger.info(f"ðŸ”¤ ä½¿ç”¨åŸºç¡€æœ¯è¯­ {len(key_terms)} ä¸ª")
                
                # æž„å»ºæœ¯è¯­çº æ­£æç¤ºï¼ˆåŒ…å«æ‹¼éŸ³æ˜ å°„ï¼‰
                terms_hint = []
                for term in key_terms:
                    if term in pinyin_map:
                        variants = ", ".join(pinyin_map[term])
                        terms_hint.append(f"  â€¢ {term} (å¯èƒ½è¢«è¯†åˆ«ä¸º: {variants})")
                    else:
                        terms_hint.append(f"  â€¢ {term}")
                
                terms_display = "\n".join(terms_hint)
                
                # Create prompt for subtitle correction
                prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ–‡å­—å¹•çº é”™åŠ©æ‰‹ã€‚è¯·çº æ­£è¯­éŸ³è¯†åˆ«é”™è¯¯ï¼Œ**ä¿æŒä¸­æ–‡å¥å­ä¸å˜ï¼Œåªä¿®æ­£è¯¯è¯†åˆ«çš„ä¸“ä¸šæœ¯è¯­**ã€‚

é‡è¦ä¸“ä¸šæœ¯è¯­ï¼ˆè¢«è¯†åˆ«æˆæ‹¼éŸ³/åŒéŸ³å­—ï¼Œå¿…é¡»è¿˜åŽŸï¼‰ï¼š
{terms_display}

çº æ­£ç¤ºä¾‹ï¼ˆç†è§£ä»»åŠ¡ï¼‰ï¼š
âŒ é”™è¯¯ï¼š"é‚£ä¹ˆè¿™ä¸ªpast apiçš„è¿™ä¸ªæœªå¤‡api"
âœ… æ­£ç¡®ï¼š"é‚£ä¹ˆè¿™ä¸ªFastAPIçš„è¿™ä¸ªWeb API"

âŒ é”™è¯¯ï¼š"æˆ‘ä»¬ä¹‹å‰ç”¨è¿‡çš„rgè¿™äº›ä¸œè¥¿"
âœ… æ­£ç¡®ï¼š"æˆ‘ä»¬ä¹‹å‰ç”¨è¿‡çš„RAGè¿™äº›ä¸œè¥¿"

âŒ é”™è¯¯ï¼š"ç”¨è¿™ä¸ªdpsi-goyleæ¨¡åž‹"
âœ… æ­£ç¡®ï¼š"ç”¨è¿™ä¸ªDeepSeek R1æ¨¡åž‹"

çº æ­£åŽŸåˆ™ï¼š
1. **åªçº æ­£ä¸“ä¸šæœ¯è¯­** - è¯†åˆ«æ‹¼éŸ³/åŒéŸ³å­—å¹¶è¿˜åŽŸè‹±æ–‡ï¼ˆpast apiâ†’FastAPI, rgâ†’RAGï¼‰
2. **ä¿æŒä¸­æ–‡å¥å­** - ä¸æ”¹å˜ä¸­æ–‡éƒ¨åˆ†ï¼Œä¸ç¿»è¯‘æˆè‹±æ–‡
3. **æœ€å°æ”¹åŠ¨** - åªä¿®æ­£æ˜Žæ˜¾é”™è¯¯ï¼Œä¸é‡å†™å¥å­
4. **ä¿æŒå£è¯­åŒ–** - ä¿ç•™"çš„è¯"ã€"å¯¹å§"ç­‰å£è¯­è¡¨è¾¾

âš ï¸ è¾“å‡ºæ ¼å¼ï¼š
- æ¯è¡Œä¸€å¥ï¼ŒæŒ‰åºå·1ã€2ã€3è¾“å‡º
- åªè¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸è¦markdownã€ä¸è¦è¯´æ˜Ž
- å¿…é¡»è¾“å‡º {len(batch)} è¡Œ

åŽŸå§‹å­—å¹•ï¼š
{full_text}

çº æ­£åŽï¼š"""
                
                logger.info(f"ðŸ“¤ Batch prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
                
                # è°ƒç”¨ LLMï¼ˆä½¿ç”¨åŒæ­¥chatæ–¹æ³•ï¼Œå†…éƒ¨ä¼šå¤„ç†äº‹ä»¶å¾ªçŽ¯ï¼‰
                response = None
                try:
                    logger.info("ðŸ”„ è°ƒç”¨ LLM provider.chat()...")
                    response = provider.chat(prompt)
                    logger.info("âœ… LLM è°ƒç”¨æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {str(e)}")
                    logger.warning("âŒ LLM çº é”™å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™çº é”™")
                    return self._correct_subtitle_with_rules(segments)
                
                if response:
                    # æ‰“å°å®Œæ•´å“åº”ç”¨äºŽè°ƒè¯•ï¼ˆé™åˆ¶é•¿åº¦é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                    logger.info(f"ðŸ“¥ LLM å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                    logger.debug(f"ðŸ“¥ LLM å®Œæ•´å“åº”:\n{response[:500]}...")  # åªæ‰“å°å‰500å­—ç¬¦
                    logger.info("=" * 80)
                    
                    # Parse corrected text - åªè§£æžæœ‰åºå·çš„è¡Œ
                    corrected_lines = []
                    import re
                    for line in response.strip().split('\n'):
                        line = line.strip()
                        # ä¸¥æ ¼åŒ¹é… "æ•°å­—. æ–‡æœ¬" æ ¼å¼
                        match = re.match(r'^(\d+)\.\s*(.+)$', line)
                        if match:
                            text = match.group(2).strip()
                            if text:  # ç¡®ä¿æ–‡æœ¬éžç©º
                                corrected_lines.append(text)
                                logger.debug(f"è§£æžè¡Œ {match.group(1)}: {text[:50]}...")
                    
                    logger.info(f"âœ… æœ¬æ‰¹æ¬¡è§£æžå¾—åˆ° {len(corrected_lines)} è¡Œçº æ­£æ–‡æœ¬ï¼ŒåŽŸå§‹æœ‰ {len(batch)} æ®µ")
                    if corrected_lines:
                        logger.info(f"å‰5è¡Œç¤ºä¾‹: {corrected_lines[:5]}")
                    
                    # Update batch segments with corrected text
                    if len(corrected_lines) == len(batch):
                        for i, corrected_text in enumerate(corrected_lines):
                            batch[i]['text'] = corrected_text
                        corrected_segments.extend(batch)
                        logger.info(f"âœ… æ‰¹æ¬¡çº æ­£æˆåŠŸï¼å·²å¤„ç† {len(corrected_segments)}/{total_segments} æ¡å­—å¹•")
                    else:
                        # è¡Œæ•°ä¸åŒ¹é…ï¼šå¯¹æœ¬æ‰¹æ¬¡ä½¿ç”¨è§„åˆ™çº é”™ï¼Œä½†ä¿ç•™å…¶ä»–æ‰¹æ¬¡çš„LLMçº é”™ç»“æžœ
                        logger.warning(f"âš ï¸ æœ¬æ‰¹æ¬¡çº æ­£è¡Œæ•° ({len(corrected_lines)}) ä¸ŽåŽŸå§‹æ®µæ•° ({len(batch)}) ä¸åŒ¹é…")
                        logger.warning(f"âš ï¸ å¯¹æœ¬æ‰¹æ¬¡ä½¿ç”¨è§„åˆ™çº é”™ï¼ˆä¿ç•™å‰ {len(corrected_segments)} æ¡å·²çº æ­£å­—å¹•ï¼‰")
                        # å¯¹å½“å‰æ‰¹æ¬¡åº”ç”¨è§„åˆ™çº é”™
                        batch_corrected = self._correct_subtitle_with_rules(batch)
                        corrected_segments.extend(batch_corrected)
                        logger.info(f"âœ… æ‰¹æ¬¡é™çº§å®Œæˆï¼å·²å¤„ç† {len(corrected_segments)}/{total_segments} æ¡å­—å¹•")
                else:
                    logger.warning("âŒ LLM å“åº”ä¸ºç©ºï¼Œä½¿ç”¨è§„åˆ™çº é”™")
                    return self._correct_subtitle_with_rules(segments)
            
            # æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆ
            logger.info(f"âœ…âœ…âœ… LLM æ™ºèƒ½çº é”™å…¨éƒ¨å®Œæˆï¼å…±çº æ­£ {len(corrected_segments)} æ¡å­—å¹•")
            logger.info("=" * 60)
            return corrected_segments
            
        except Exception as e:
            logger.error(f"Failed to correct subtitles with LLM: {e}, falling back to rule-based correction")
            return self._correct_subtitle_with_rules(segments)

    
    def _correct_subtitle_with_rules(self, segments: list) -> list:
        """Use rule-based correction for common errors"""
        # Common homophone errors in Chinese speech recognition
        correction_rules = {
            # Technology terms - First pass (specific phrases)
            'äºšä¼¯æ™ºèƒ½æœ€æ–°å–„ä»·çš„æ•°ç æ´¾å®˜æ–¹ç¼…ç§°è‰²æ½œå¤´å¤œå¸‚ç‰ˆ': 'æ ‘èŽ“æ´¾æœ€æ–°ä¸Šæž¶çš„æ ‘èŽ“æ´¾å®˜æ–¹æ‘„åƒå¤´å¤œè§†ç‰ˆ',
            'æ•°ç æ´¾å®˜æ–¹ç¼…ç§°è‰²æ½œå¤´': 'æ ‘èŽ“æ´¾å®˜æ–¹æ‘„åƒå¤´',
            'å®˜æ–¹ç¼…ç§°è‰²æ½œå¤´': 'å®˜æ–¹æ‘„åƒå¤´',
            'æ•°ç æ€»ç»Ÿç‰ˆçš„è‰²æ½œå¤´æ–¹å‘æ“æŽ§': 'æ ‘èŽ“æ´¾çš„æ‘„åƒå¤´æŽ¥å£',
            'è‰²æ½œå¤´æ–¹å‘æ“æŽ§': 'æ‘„åƒå¤´æŽ¥å£',
            'æ•°ç æ€»ç»Ÿç‰ˆ': 'æ ‘èŽ“æ´¾',
            'æ•°ç ç³»ç»Ÿ': 'æ ‘èŽ“æ´¾ç³»ç»Ÿ',
            'æœ€æ–°å–„ä»·': 'æœ€æ–°ä¸Šæž¶',
            'å–„ä»·çš„': 'ä¸Šæž¶çš„',
            'è‰²æ½œå¤´': 'æ‘„åƒå¤´',
            'å¤œå¸‚ç‰ˆ': 'å¤œè§†ç‰ˆ',
            'è‰²äº§ä¸šçš„': 'æ‘„åƒçš„',
            'è‰²äº§ä¸š': 'æ‘„åƒ',
            'æµæ°”æ°”': 'æµè§ˆå™¨',
            'æµå™¨æ°”': 'æµè§ˆå™¨',
            'æ•°ç æ´¾': 'æ ‘èŽ“æ´¾',
            'è£…ç”¨è‰²æ½œå¤´': 'ä¸“ç”¨æ‘„åƒå¤´',
            'è£…ç”¨': 'ä¸“ç”¨',
            'äºšä¼¯æ™ºèƒ½': 'æ ‘èŽ“æ´¾',
            'è§†é¢‘IPåœ°å€': 'IPåœ°å€',
            
            # Second pass (individual words)
            'å–„ä»·': 'ä¸Šæž¶',
            'ç¼…ç§°': 'æ‘„åƒ',
            'æ½œå¤´': 'åƒå¤´',
            'æ“¦å…¥': 'æ’å…¥',
            'å¤œå¸‚': 'å¤œè§†',
            'æ€»ç»Ÿç‰ˆ': 'æ´¾',
            'æ€»ç»Ÿ': 'æ´¾',
            'è®°å¿†ä¸Š': 'ç•Œé¢ä¸Š',
            'è®°å¿†': 'ç•Œé¢',
            'è§†é¢‘IP': 'IP',
            
            # Common misrecognitions
            'æ°”æ°”': 'å™¨',
            'æ°”': 'å™¨',
            'ç”£': 'äº§',
        }
        
        logger.info("Applying rule-based subtitle correction...")
        corrected_count = 0
        
        for segment in segments:
            original_text = segment['text']
            corrected_text = original_text
            
            # Apply correction rules (ordered by length to handle phrases first)
            sorted_rules = sorted(correction_rules.items(), key=lambda x: len(x[0]), reverse=True)
            for wrong, correct in sorted_rules:
                if wrong in corrected_text:
                    before = corrected_text
                    corrected_text = corrected_text.replace(wrong, correct)
                    if before != corrected_text:
                        corrected_count += 1
                        logger.debug(f"Applied rule: '{wrong}' -> '{correct}'")
            
            # Update segment if changed
            if corrected_text != original_text:
                segment['text'] = corrected_text
                logger.debug(f"Corrected: '{original_text}' -> '{corrected_text}'")
        
        logger.info(f"Rule-based correction applied {corrected_count} fixes to {len(segments)} segments")
        return segments
    
    def _extract_audio(self, video_path: str, audio_path: str) -> bool:
        """Extract audio from video"""
        try:
            cmd = [
                "-i", video_path,
                "-vn",  # No video
                "-acodec", "pcm_s16le",  # PCM audio
                "-ar", "16000",  # 16kHz sample rate for Whisper
                "-ac", "1",  # Mono
                audio_path,
                "-y"
            ]
            result = self._run_ffmpeg(cmd)
            return result.get("success", False)
        except Exception as e:
            logger.error(f"Failed to extract audio: {e}")
            return False
    
    def _format_timestamp(self, seconds: float) -> str:
        """Format seconds to SRT timestamp format"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds - int(seconds)) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"
    
    def _convert_to_simplified_chinese(self, text: str) -> str:
        """Convert traditional Chinese to simplified Chinese"""
        try:
            from opencc import OpenCC
            cc = OpenCC('t2s')  # Traditional to Simplified
            return cc.convert(text)
        except ImportError:
            # If OpenCC is not available, try using a simple mapping
            logger.warning("OpenCC not installed, using basic conversion")
            # Basic traditional to simplified mapping
            trans_map = {
                'æ­¡è¿Ž': 'æ¬¢è¿Ž', 'è§€çœ‹': 'è§‚çœ‹', 'æ•¸ç¢¼': 'æ•°ç ', 'ç·¬ç¨±': 'ç¼…ç§°',
                'æ“¦å…¥': 'æ’å…¥', 'ç¸½çµ±': 'æ€»ç»Ÿ', 'ç¹¼çºŒ': 'ç»§ç»­', 'è®“': 'è®©',
                'é‹è¡Œ': 'è¿è¡Œ', 'æ°£': 'å™¨', 'è¨˜æ†¶': 'ç•Œé¢', 'ç”¢æ¥­': 'æ‘„åƒ',
                'çµæŸ': 'ç»“æŸ', 'è¬è¬': 'è°¢è°¢', 'åƒ¹': 'ä»·', 'è¦–é »': 'è§†é¢‘',
                'æ“¦': 'æ’', 'ç¶«': 'çº¿', 'æ½›é ­': 'æ‘„åƒå¤´', 'å¤œå¸‚': 'å¤œè§†'
            }
            for trad, simp in trans_map.items():
                text = text.replace(trad, simp)
            return text
        except Exception as e:
            logger.warning(f"Conversion failed: {e}, using original text")
            return text
    
    def _translate_to_english(self, text: str) -> str:
        """Translate Chinese text to English using LLM"""
        llm_manager = self._get_llm_manager()
        if not llm_manager:
            logger.warning("LLM not available for translation")
            # Return a placeholder that indicates translation is needed
            return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
        
        try:
            # Get provider by task type
            provider = llm_manager.get_provider(task_type="subtitle_translation")
            if not provider:
                logger.warning("No translation provider available")
                # Return Chinese text as fallback
                return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
            
            prompt = f"""Translate the following Chinese subtitle to natural English. Keep it concise and suitable for video subtitles.
Only output the English translation, no explanations.

Chinese: {text}
English:"""
            
            try:
                response = provider.chat(prompt)
                if response:
                    return response.strip()
                else:
                    return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
            except Exception as e:
                logger.warning(f"Translation API call failed: {e}")
                return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
        except Exception as e:
            logger.error(f"Translation failed: {e}")
            return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
    
    def _write_srt(self, segments: list, output_path: str, convert_to_simplified: bool = True, bilingual: bool = False) -> bool:
        """Write segments to SRT file
        
        Args:
            segments: List of subtitle segments
            output_path: Output SRT file path
            convert_to_simplified: Convert traditional Chinese to simplified
            bilingual: Generate bilingual (Chinese + English) subtitles
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                for i, segment in enumerate(segments, 1):
                    start = self._format_timestamp(segment['start'])
                    end = self._format_timestamp(segment['end'])
                    text = segment['text'].strip()
                    
                    # Convert to simplified Chinese if requested
                    if convert_to_simplified:
                        text = self._convert_to_simplified_chinese(text)
                    
                    f.write(f"{i}\n")
                    f.write(f"{start} --> {end}\n")
                    
                    if bilingual:
                        # Write Chinese and English on separate lines
                        english_text = segment.get('english_text', '')
                        if not english_text:
                            # Translate if not already translated
                            english_text = self._translate_to_english(text)
                            segment['english_text'] = english_text
                        
                        f.write(f"{text}\n")
                        f.write(f"{english_text}\n\n")
                    else:
                        f.write(f"{text}\n\n")
            return True
        except Exception as e:
            logger.error(f"Failed to write SRT file: {e}")
            return False
    
    def execute(
        self,
        video_path: str,
        language: str = "zh",
        output_path: Optional[str] = None,
        model: str = "base",
        embed_subtitle: bool = False,
        bilingual: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """Generate subtitle file using Whisper and optionally embed it into video
        
        Args:
            video_path: Path to input video file
            language: Language code for speech recognition (default: zh)
            output_path: Path to output file
            model: Whisper model to use (default: base)
            embed_subtitle: Whether to burn subtitle into video
            bilingual: Generate bilingual Chinese+English subtitles
            **kwargs: Additional arguments (use_llm_correction, etc.)
        """
        if not validate_video_file(video_path):
            return {"success": False, "error": "Invalid video file"}
        
        # Determine subtitle file path
        if output_path is None:
            if embed_subtitle:
                # For embedded subtitle, output is a video file
                base = str(Path(video_path).stem)
                output_path = str(Path(video_path).parent / f"{base}-subtitle.avi")
            else:
                # For standalone subtitle, output is SRT file
                output_path = str(Path(video_path).with_suffix('.srt'))
        else:
            output_path = str(ensure_output_dir(output_path))
        
        # Generate SRT file path
        if embed_subtitle:
            # åŸºäºŽè¾“å‡ºæ–‡ä»¶è·¯å¾„ç”Ÿæˆ SRTï¼Œé¿å…è¦†ç›–æºæ–‡ä»¶çš„ SRT
            srt_path = str(Path(output_path).with_suffix('.srt'))
        else:
            # ä¸åµŒå…¥æ—¶ï¼Œå¼ºåˆ¶ä½¿ç”¨.srtåŽç¼€ï¼ˆå³ä½¿ä¼ å…¥çš„æ˜¯.mp4ï¼‰
            srt_path = str(Path(output_path).with_suffix('.srt'))
        
        try:
            # ç¡®ä¿ ffmpeg åœ¨ PATH ä¸­ï¼ˆWhisper éœ€è¦è°ƒç”¨ ffmpegï¼‰
            ffmpeg_dir = str(Path(self.ffmpeg_path).parent)
            if ffmpeg_dir not in os.environ.get('PATH', ''):
                os.environ['PATH'] = ffmpeg_dir + os.pathsep + os.environ.get('PATH', '')
                logger.info(f"Added ffmpeg directory to PATH: {ffmpeg_dir}")
            
            # Step 1: Extract audio from video
            logger.info(f"Extracting audio from {video_path}")
            audio_path = str(Path(video_path).with_suffix('.wav'))
            if not self._extract_audio(video_path, audio_path):
                return {"success": False, "error": "Failed to extract audio"}
            
            # Step 2: Transcribe audio using Whisper
            logger.info(f"Transcribing audio with Whisper (language: {language})")
            model_obj = self._get_whisper_model(model)
            result = model_obj.transcribe(audio_path, language=language)
            
            # Step 2.5: Auto-extract technical terms if not provided
            segments = result['segments']
            tech_terms = kwargs.get('tech_terms', {})
            
            if not tech_terms and kwargs.get('auto_extract_terms', True):
                logger.info("è‡ªåŠ¨æå–ä¸“ä¸šæœ¯è¯­...")
                # ä»Žåˆæ­¥å­—å¹•ä¸­æå–ä¸“ä¸šæœ¯è¯­
                initial_text = "\n".join([seg['text'] for seg in segments])
                extracted_terms = self._extract_technical_terms(video_path, initial_text)
                if extracted_terms:
                    tech_terms = extracted_terms
                    kwargs['tech_terms'] = tech_terms
                    logger.info(f"è‡ªåŠ¨æå–äº† {len(tech_terms)} ä¸ªä¸“ä¸šæœ¯è¯­")
            
            # First convert to simplified Chinese
            for seg in segments:
                seg['text'] = self._convert_to_simplified_chinese(seg['text'])
            
            # Then apply corrections
            correction_count = 0
            if kwargs.get('use_llm_correction', True):
                logger.info("Applying LLM-based subtitle correction...")
                segments = self._correct_subtitle_with_llm(segments, **kwargs)
                # è®¡ç®—çº æ­£æ•°é‡ï¼ˆç®€å•ä¼°ç®—ä¸ºä½¿ç”¨LLMçº æ­£çš„æ®µæ•°ï¼‰
                correction_count = len(segments)
            else:
                # å³ä½¿ä¸ç”¨LLMï¼Œä¹Ÿåº”ç”¨è§„åˆ™çº é”™
                logger.info("Applying rule-based subtitle correction...")
                segments = self._correct_subtitle_with_rules(segments)
            
            # Step 2.6: Translate to English if bilingual mode
            if bilingual:
                logger.info("Translating subtitles to English for bilingual mode...")
                for seg in segments:
                    if 'english_text' not in seg:
                        seg['english_text'] = self._translate_to_english(seg['text'])
            
            # Step 3: Write SRT file (skip conversion since already done)
            logger.info(f"Writing subtitle to {srt_path}")
            if not self._write_srt(segments, srt_path, convert_to_simplified=False, bilingual=bilingual):
                return {"success": False, "error": "Failed to write SRT file"}
            
            # Clean up audio file
            if os.path.exists(audio_path):
                os.remove(audio_path)
            
            # Step 4: Embed subtitle if requested (ä½¿ç”¨è½¯å­—å¹•æµï¼Œä¸çƒ§å½•åˆ°ç”»é¢)
            if embed_subtitle:
                logger.info(f"Embedding subtitle stream (soft subtitle) into video: {video_path}")
                
                # Convert to absolute paths
                abs_video_path = str(Path(video_path).absolute())
                abs_srt_path = str(Path(srt_path).absolute())
                
                # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
                temp_output = str(Path(video_path).parent / f"{Path(video_path).stem}_temp{Path(video_path).suffix}")
                abs_temp_output = str(Path(temp_output).absolute())
                
                # ä½¿ç”¨è½¯å­—å¹•ï¼šå°†å­—å¹•æµåµŒå…¥è§†é¢‘å®¹å™¨ï¼Œä¸é‡æ–°ç¼–ç è§†é¢‘
                logger.info(f"Adding subtitle stream using: {abs_srt_path}")
                cmd = [
                    "-i", abs_video_path,
                    "-i", abs_srt_path,
                    "-c:v", "copy",  # è§†é¢‘æµç›´æŽ¥å¤åˆ¶ï¼Œä¸é‡æ–°ç¼–ç 
                    "-c:a", "copy",  # éŸ³é¢‘æµç›´æŽ¥å¤åˆ¶
                    "-c:s", "mov_text",  # å­—å¹•ç¼–ç æ ¼å¼ï¼ˆMP4ç”¨mov_textï¼‰
                    "-metadata:s:s:0", "language=chi",  # è®¾ç½®å­—å¹•è¯­è¨€
                    "-metadata:s:s:0", "title=Chinese",  # è®¾ç½®å­—å¹•æ ‡é¢˜
                    abs_temp_output,
                    "-y"
                ]
                
                embed_result = self._run_ffmpeg(cmd)
                
                if not embed_result.get("success", False):
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(temp_output):
                        os.remove(temp_output)
                    return {
                        "success": False,
                        "error": f"Failed to embed subtitle: {embed_result.get('error', 'Unknown error')}",
                        "srt_path": srt_path
                    }
                
                # æˆåŠŸåŽï¼ŒåŽŸåœ°æ›¿æ¢æºæ–‡ä»¶
                try:
                    # åˆ é™¤æºæ–‡ä»¶
                    os.remove(video_path)
                    # é‡å‘½åä¸´æ—¶æ–‡ä»¶ä¸ºæºæ–‡ä»¶å
                    os.rename(temp_output, video_path)
                    logger.info(f"Successfully replaced source file with subtitled version: {video_path}")
                except Exception as e:
                    return {
                        "success": False,
                        "error": f"Failed to replace source file: {str(e)}",
                        "srt_path": srt_path
                    }
                
                return {
                    "success": True,
                    "output_path": video_path,  # è¿”å›žæºæ–‡ä»¶è·¯å¾„ï¼ˆå·²è¢«æ›¿æ¢ï¼‰
                    "srt_path": srt_path,
                    "extracted_terms": tech_terms,
                    "correction_count": correction_count,
                    "message": f"Subtitle embedded into source file successfully (soft subtitle, original encoding preserved)"
                }
            else:
                return {
                    "success": True,
                    "output_path": srt_path,
                    "srt_path": srt_path,
                    "extracted_terms": tech_terms,
                    "correction_count": correction_count,
                    "message": f"Subtitle generated successfully"
                }
            
        except Exception as e:
            logger.error(f"Subtitle generation failed: {e}", exc_info=True)
            # Clean up temporary files
            for tmp_file in [audio_path, srt_path if embed_subtitle else None]:
                if tmp_file and os.path.exists(tmp_file):
                    try:
                        os.remove(tmp_file)
                    except:
                        pass
            return {"success": False, "error": str(e)}


class FormatTool(MediaTool):
    """Tool for format conversion"""
    
    def execute(
        self,
        input_path: str,
        output_format: str,
        output_path: str,
        resolution: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Convert video format"""
        if not validate_video_file(input_path):
            return {"success": False, "error": "Invalid video file"}
        
        output_path = str(ensure_output_dir(output_path))
        
        cmd = ["-i", input_path]
        
        # Add resolution filter if specified
        if resolution:
            if resolution == "1080p":
                cmd.extend(["-vf", "scale=1920:1080"])
            elif resolution == "720p":
                cmd.extend(["-vf", "scale=1280:720"])
            elif resolution == "480p":
                cmd.extend(["-vf", "scale=854:480"])
        
        # Add output format and path
        cmd.extend(["-c:v", "libx264", "-c:a", "aac", output_path, "-y"])
        
        return self._run_ffmpeg(cmd)


class ImageTool:
    """Tool for image processing with smart rotation based on person detection"""
    
    def __init__(self):
        if not PIL_AVAILABLE:
            raise ImportError("Pillow is not installed. Install it with: pip install Pillow")
        
        # Initialize YOLO model for person detection (lazy loading)
        self._yolo_model = None
        self._yolo_available = YOLO_AVAILABLE
    
    def _get_yolo_model(self):
        """Get or initialize YOLO model for person detection"""
        if not self._yolo_available:
            return None
        
        if self._yolo_model is None:
            try:
                # Use YOLOv8n (nano) for faster inference, can detect person class (class 0)
                self._yolo_model = YOLO('yolov8n.pt')
                logger.info("YOLO model loaded successfully")
            except Exception as e:
                logger.warning(f"Failed to load YOLO model: {e}. Smart rotation will be disabled.")
                self._yolo_available = False
                return None
        
        return self._yolo_model
    
    def _detect_person_and_analyze_orientation(self, img: Image.Image) -> Dict[str, Any]:
        """Detect person in image and analyze orientation"""
        if not self._yolo_available:
            return {"detected": False, "rotation_needed": 0}
        
        model = self._get_yolo_model()
        if model is None:
            return {"detected": False, "rotation_needed": 0}
        
        try:
            # Convert PIL to numpy array for YOLO
            img_array = np.array(img)
            
            # Run detection (person class is 0 in COCO dataset)
            results = model(img_array, classes=[0], verbose=False)  # Only detect person class
            
            if len(results) == 0 or len(results[0].boxes) == 0:
                return {"detected": False, "rotation_needed": 0}
            
            # Get the largest person bounding box (most prominent person)
            boxes = results[0].boxes
            largest_box = None
            largest_area = 0
            
            img_width, img_height = img.size
            
            for box in boxes:
                # Get box coordinates
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                area = (x2 - x1) * (y2 - y1)
                
                if area > largest_area:
                    largest_area = area
                    largest_box = {
                        "x1": float(x1),
                        "y1": float(y1),
                        "x2": float(x2),
                        "y2": float(y2),
                        "width": float(x2 - x1),
                        "height": float(y2 - y1),
                        "center_x": float((x1 + x2) / 2),
                        "center_y": float((y1 + y2) / 2),
                    }
            
            if largest_box is None:
                return {"detected": False, "rotation_needed": 0}
            
            # Analyze person orientation
            person_width = largest_box["width"]
            person_height = largest_box["height"]
            person_aspect_ratio = person_width / person_height if person_height > 0 else 1.0
            
            # Analyze image orientation
            image_aspect_ratio = img_width / img_height if img_height > 0 else 1.0
            
            # Determine if rotation is needed
            rotation_needed = 0
            
            # If image is landscape (wide) and person is also wide (horizontal)
            # Or if person's bounding box is wider than tall, person might be horizontal
            if image_aspect_ratio > 1.0:  # Landscape image
                if person_aspect_ratio > 1.2:  # Person is wider than tall (horizontal)
                    # Person is horizontal in landscape image, rotate 90 degrees CCW
                    rotation_needed = 90
                elif person_aspect_ratio < 0.8:  # Person is taller than wide (vertical)
                    # Person is already vertical, might need -90 rotation
                    rotation_needed = -90
            else:  # Portrait image
                if person_aspect_ratio > 1.2:  # Person is wider than tall
                    # Person is horizontal in portrait, rotate 90 degrees
                    rotation_needed = 90
            
            logger.info(f"Person detected: aspect_ratio={person_aspect_ratio:.2f}, rotation_needed={rotation_needed}")
            
            return {
                "detected": True,
                "rotation_needed": rotation_needed,
                "person_box": largest_box,
                "person_aspect_ratio": person_aspect_ratio,
                "image_aspect_ratio": image_aspect_ratio
            }
        except Exception as e:
            logger.warning(f"YOLO detection failed: {e}")
            return {"detected": False, "rotation_needed": 0}
    
    def execute(
        self,
        input_path: str,
        output_path: str,
        width: Optional[int] = None,
        height: Optional[int] = None,
        aspect_ratio: Optional[str] = None,  # e.g., "9:16" for vertical
        resize_mode: str = "fit",  # "fit"(pad), "cover"(fill+crop), "stretch"
        smart_rotate: bool = True,  # Enable smart rotation based on person detection
        **kwargs
    ) -> Dict[str, Any]:
        """Process image: resize, convert format, etc."""
        try:
            if not os.path.exists(input_path):
                return {"success": False, "error": f"Input file not found: {input_path}"}
            
            # Open image
            img = Image.open(input_path)
            original_width, original_height = img.size
            
            # Smart rotation: detect person and rotate if needed
            rotation_applied = 0
            if smart_rotate and (aspect_ratio in ("9:16", "vertical") or (width and height and height > width)):
                detection_result = self._detect_person_and_analyze_orientation(img)
                if detection_result.get("detected") and detection_result.get("rotation_needed") != 0:
                    rotation_needed = detection_result["rotation_needed"]
                    # Rotate image
                    if rotation_needed == 90:
                        img = img.rotate(-90, expand=True)  # Rotate counter-clockwise
                        rotation_applied = 90
                    elif rotation_needed == -90:
                        img = img.rotate(90, expand=True)  # Rotate clockwise
                        rotation_applied = -90
                    # Update dimensions after rotation
                    original_width, original_height = img.size
                    logger.info(f"Applied rotation: {rotation_applied} degrees")
            
            # Determine target dimensions
            target_width = width
            target_height = height
            
            # If aspect ratio is specified, calculate dimensions
            if aspect_ratio and not (width and height):
                if aspect_ratio == "9:16" or aspect_ratio == "vertical":
                    # Common phone vertical aspect ratio
                    if width:
                        target_height = int(width * 16 / 9)
                    elif height:
                        target_width = int(height * 9 / 16)
                    else:
                        # Use common phone vertical size: 1080x1920
                        target_width = 1080
                        target_height = 1920
                elif ":" in aspect_ratio:
                    ratio_parts = aspect_ratio.split(":")
                    ratio_w = float(ratio_parts[0])
                    ratio_h = float(ratio_parts[1])
                    if width:
                        target_height = int(width * ratio_h / ratio_w)
                    elif height:
                        target_width = int(height * ratio_w / ratio_h)
                    else:
                        # Default to 1080 width
                        target_width = 1080
                        target_height = int(1080 * ratio_h / ratio_w)
            
            # If no dimensions specified, use defaults
            if not target_width and not target_height:
                target_width = 1080
                target_height = 1920
            
            # Ensure output directory exists
            output_path = str(ensure_output_dir(output_path))
            
            # Normalize and validate resize_mode
            resize_mode = (resize_mode or "fit").lower().strip()
            if resize_mode == "crop":
                # Backward-compat alias: crop behaves like cover (fill+crop)
                resize_mode = "cover"

            # Resize image based on mode
            if resize_mode == "fit":
                # Fit image maintaining aspect ratio, add padding if needed (letterbox)
                img.thumbnail((target_width, target_height), Image.Resampling.LANCZOS)
                new_img = Image.new("RGB", (target_width, target_height), (255, 255, 255))
                paste_x = (target_width - img.size[0]) // 2
                paste_y = (target_height - img.size[1]) // 2
                new_img.paste(img, (paste_x, paste_y))
                img = new_img
            elif resize_mode == "cover":
                # Cover target size while preserving aspect ratio, then center-crop (no blank bars)
                ow, oh = img.size
                scale = max(target_width / ow, target_height / oh)
                new_w = max(1, int(round(ow * scale)))
                new_h = max(1, int(round(oh * scale)))
                img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)

                left = max(0, (new_w - target_width) // 2)
                top = max(0, (new_h - target_height) // 2)
                right = left + target_width
                bottom = top + target_height
                img = img.crop((left, top, right, bottom))
            else:  # stretch
                # Stretch to exact dimensions (may distort)
                img = img.resize((target_width, target_height), Image.Resampling.LANCZOS)
            
            # Convert to RGB if necessary (for JPEG compatibility)
            if img.mode != "RGB" and output_path.lower().endswith(('.jpg', '.jpeg')):
                img = img.convert("RGB")
            
            # Save image
            img.save(output_path, quality=95)
            
            result = {
                "success": True,
                "output_path": output_path,
                "original_size": f"{original_width}x{original_height}",
                "new_size": f"{target_width}x{target_height}"
            }
            
            if rotation_applied != 0:
                result["rotation_applied"] = rotation_applied
                result["smart_rotation"] = True
            
            return result
        except Exception as e:
            return {"success": False, "error": str(e)}


class OptimizeTool(MediaTool):
    """Tool for audio/video optimization"""
    
    def execute(
        self,
        input_path: str,
        output_path: str,
        optimize_type: str = "audio",  # "audio" or "video"
        **kwargs
    ) -> Dict[str, Any]:
        """Optimize audio or video"""
        if not validate_video_file(input_path):
            return {"success": False, "error": "Invalid video file"}
        
        output_path = str(ensure_output_dir(output_path))
        
        cmd = ["-i", input_path]
        
        if optimize_type == "audio":
            # Audio denoising and normalization
            cmd.extend([
                "-af", "highpass=f=200,lowpass=f=3000,volume=1.5",
                "-c:v", "copy",  # Copy video stream
                output_path,
                "-y"
            ])
        elif optimize_type == "video":
            # Video enhancement (basic)
            cmd.extend([
                "-vf", "eq=contrast=1.2:brightness=0.05:saturation=1.1",
                "-c:a", "copy",  # Copy audio stream
                output_path,
                "-y"
            ])
        else:
            return {"success": False, "error": f"Unknown optimize_type: {optimize_type}"}
        
        return self._run_ffmpeg(cmd)
