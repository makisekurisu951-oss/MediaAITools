"""MCP Tools for media processing"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import subprocess
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from utils.media_utils import validate_video_file, parse_time, format_time, ensure_output_dir
from utils.logger import get_logger

logger = get_logger(__name__)


def run_async_in_new_loop(coro, timeout=30):
    """åœ¨æ–°çº¿ç¨‹çš„äº‹ä»¶å¾ªçŽ¯ä¸­è¿è¡Œå¼‚æ­¥ä»£ç ï¼ˆé¿å…åµŒå¥—äº‹ä»¶å¾ªçŽ¯å†²çªï¼‰
    
    Args:
        coro: å¼‚æ­¥åç¨‹å¯¹è±¡
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
        
    Returns:
        åç¨‹çš„è¿”å›žå€¼ï¼Œæˆ–åœ¨è¶…æ—¶æ—¶æŠ›å‡º TimeoutError
    """
    def run_in_thread():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(coro)
        finally:
            loop.close()
    
    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(run_in_thread)
        try:
            return future.result(timeout=timeout)
        except Exception as e:
            logger.error(f"Async task failed or timeout after {timeout}s: {e}")
            raise

# Image processing imports
try:
    from PIL import Image, ImageOps
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None
    ImageOps = None

# YOLO imports for object detection
try:
    from ultralytics import YOLO
    import numpy as np
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    YOLO = None
    np = None


class MediaTool(ABC):
    """Base class for media processing tools"""
    
    def __init__(self, ffmpeg_path: str = "ffmpeg", ffprobe_path: str = "ffprobe"):
        self.ffmpeg_path = ffmpeg_path
        self.ffprobe_path = ffprobe_path
    
    @abstractmethod
    def execute(self, **kwargs) -> Dict[str, Any]:
        """Execute the tool"""
        pass
    
    def _run_ffmpeg(self, cmd: list, timeout: int = 300) -> Dict[str, Any]:
        """Run FFmpeg command"""
        try:
            result = subprocess.run(
                [self.ffmpeg_path] + cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                check=True,
                encoding='utf-8',
                errors='ignore'  # å¿½ç•¥ç¼–ç é”™è¯¯
            )
            return {"success": True, "output": result.stdout}
        except subprocess.TimeoutExpired:
            return {"success": False, "error": "Command timeout"}
        except subprocess.CalledProcessError as e:
            return {"success": False, "error": e.stderr}
        except Exception as e:
            return {"success": False, "error": str(e)}


class ClipTool(MediaTool):
    """Tool for video clipping"""
    
    def execute(
        self,
        input_path: str,
        start_time: str,
        end_time: str,
        output_path: str,
        **kwargs
    ) -> Dict[str, Any]:
        """Clip video from start_time to end_time"""
        if not validate_video_file(input_path):
            return {"success": False, "error": "Invalid video file"}
        
        output_path = str(ensure_output_dir(output_path))
        
        start_seconds = parse_time(start_time)
        end_seconds = parse_time(end_time)
        duration = end_seconds - start_seconds
        
        cmd = [
            "-i", input_path,
            "-ss", str(start_seconds),
            "-t", str(duration),
            "-c", "copy",  # Copy codec for faster processing
            "-avoid_negative_ts", "make_zero",
            output_path,
            "-y"  # Overwrite output file
        ]
        
        return self._run_ffmpeg(cmd)


class ConcatTool(MediaTool):
    """Tool for concatenating videos"""
    
    def execute(
        self,
        video_paths: list,
        output_path: str,
        **kwargs
    ) -> Dict[str, Any]:
        """Concatenate multiple videos"""
        # Validate all input files
        for path in video_paths:
            if not validate_video_file(path):
                return {"success": False, "error": f"Invalid video file: {path}"}
        
        output_path = str(ensure_output_dir(output_path))
        
        # Create concat file list
        concat_file = output_path + ".concat.txt"
        with open(concat_file, 'w', encoding='utf-8') as f:
            for path in video_paths:
                f.write(f"file '{os.path.abspath(path)}'\n")
        
        try:
            cmd = [
                "-f", "concat",
                "-safe", "0",
                "-i", concat_file,
                "-c", "copy",
                output_path,
                "-y"
            ]
            
            result = self._run_ffmpeg(cmd)
            
            # Clean up concat file
            if os.path.exists(concat_file):
                os.remove(concat_file)
            
            return result
        except Exception as e:
            # Clean up concat file on error
            if os.path.exists(concat_file):
                os.remove(concat_file)
            return {"success": False, "error": str(e)}


class SubtitleTool(MediaTool):
    """Tool for subtitle generation using Whisper"""
    
    def __init__(self, ffmpeg_path: str = "ffmpeg", ffprobe_path: str = "ffprobe"):
        super().__init__(ffmpeg_path, ffprobe_path)
        self._whisper_model = None
        self._llm_manager = None
    
    def _get_whisper_model(self, model_name: str = "base"):
        """Lazy load Whisper model"""
        if self._whisper_model is None:
            try:
                import whisper
                import torch
                logger.info(f"Loading Whisper model: {model_name}")
                
                
                # Whisper åœ¨ GPU ä¸Šè¿è¡Œ
                device = "cuda" if torch.cuda.is_available() else "cpu"
                logger.info(f"Whisper using {device}")
                
                self._whisper_model = whisper.load_model(model_name, device=device)
                logger.info(f"Whisper model loaded successfully on {device}")
            except Exception as e:
                logger.error(f"Failed to load Whisper model: {e}")
                raise
        return self._whisper_model
    
    def _get_llm_manager(self):
        """Lazy load LLM manager for subtitle correction"""
        if self._llm_manager is None:
            try:
                import sys
                from pathlib import Path
                # Ensure llm module can be imported
                parent_dir = Path(__file__).parent.parent
                if str(parent_dir) not in sys.path:
                    sys.path.insert(0, str(parent_dir))
                
                from llm import LLMManager
                self._llm_manager = LLMManager()
                logger.info("LLM Manager initialized for subtitle correction")
            except Exception as e:
                logger.warning(f"Failed to initialize LLM Manager: {e}")
                self._llm_manager = None
        return self._llm_manager
    
    def _extract_technical_terms(self, video_path: str, initial_text: str) -> dict:
        """è‡ªåŠ¨ä»Žè§†é¢‘æ–‡ä»¶åå’Œåˆæ­¥å­—å¹•ä¸­æå–ä¸“ä¸šæœ¯è¯­"""
        # å…ˆä»Žæ–‡ä»¶åæå–å…³é”®æœ¯è¯­ï¼ˆå¿«é€Ÿä¸”å‡†ç¡®ï¼‰
        import os
        import re
        filename = os.path.basename(video_path)
        
        # ä»Žæ–‡ä»¶åä¸­æå–ä¸­æ–‡å…³é”®è¯ï¼ˆ2-4ä¸ªå­—çš„è¯ç»„ï¼‰
        filename_terms = {}
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', filename)
        for word in chinese_words:
            # è¿‡æ»¤å¸¸è§æ— æ„ä¹‰è¯
            if word not in ['è§†é¢‘', 'æ¼”ç¤º', 'å®˜æ–¹', 'åŽŸè£…', 'æ–‡ä»¶']:
                filename_terms[word] = word
        
        logger.info(f"ðŸ“ ä»Žæ–‡ä»¶åæå–å…³é”®è¯: {list(filename_terms.keys())}")
        
        # âš ï¸ Windows äº‹ä»¶å¾ªçŽ¯é™åˆ¶ï¼šæ–°çº¿ç¨‹ä¸­çš„ LLM è°ƒç”¨ä¸ç¨³å®š
        # ç›´æŽ¥è¿”å›žæ–‡ä»¶åæœ¯è¯­ï¼Œä¸è°ƒç”¨ LLMï¼ˆé¿å… Event loop closed é”™è¯¯ï¼‰
        logger.info(f"âœ… ä½¿ç”¨æ–‡ä»¶åæœ¯è¯­ {len(filename_terms)} ä¸ªï¼ˆè·³è¿‡ LLM æå–ä»¥é¿å…äº‹ä»¶å¾ªçŽ¯å†²çªï¼‰")
        return filename_terms
        
        # èŽ·å– LLM Manager
        llm_manager = self._get_llm_manager()
        if not llm_manager:
            logger.warning("LLM not available, cannot extract technical terms")
            return filename_terms
        
        provider = llm_manager.get_provider(task_type="chinese_processing")
        if not provider:
            provider = llm_manager.get_provider()
        
        if not provider:
            return filename_terms
        
        try:
            
            # å–å‰ 500 å­—ä½œä¸ºæ ·æœ¬ï¼ˆé¿å…å¤ªé•¿ï¼‰
            sample_text = initial_text[:500] if len(initial_text) > 500 else initial_text
            
            prompt = f"""è­¦å‘Š:è¿™æ˜¯è¯­éŸ³è¯†åˆ«çš„é”™è¯¯ç»“æžœï¼Œè¯·åˆ†æžå¹¶æŽ¨æ–­æ­£ç¡®çš„ä¸“ä¸šæœ¯è¯­ã€‚
ã€èƒŒæ™¯ã€‘
è§†é¢‘æ–‡ä»¶åï¼š{filename}
è¯­éŸ³è¯†åˆ«é”™è¯¯æ–‡æœ¬ï¼ˆå‰500å­—ï¼‰ï¼š
{sample_text}

ã€ä»»åŠ¡ã€‘
ä»Žä¸Šè¿°é”™è¯¯è¯†åˆ«æ–‡æœ¬å’Œæ–‡ä»¶åä¸­æŽ¨æ–­å‡ºæ­£ç¡®çš„ä¸“ä¸šæœ¯è¯­ã€‚

ã€åˆ†æžçº¿ç´¢ã€‘
1. æ–‡ä»¶åçº¿ç´¢ï¼šä»Žæ–‡ä»¶åä¸­æ‰¾å…³é”®è¯ï¼ˆå¦‚"æ ‘èŽ“æ´¾"ã€"æ‘„åƒå¤´"ç­‰ï¼‰
2. åŒéŸ³å­—æ›¿æ¢ï¼šè¯†åˆ«æ‹¼éŸ³/åŒéŸ³å­—é”™è¯¯
   - "æ•°ç æ´¾/æ•¸ç¢¼æ´¾" -> "æ ‘èŽ“æ´¾" ï¼ˆæ–‡ä»¶åæœ‰"æ ‘èŽ“æ´¾"ï¼‰
   - "è‰²æ½œå¤´/è‰²æ½›é ­" -> "æ‘„åƒå¤´" ï¼ˆæ–‡ä»¶åæœ‰"æ‘„åƒå¤´"ï¼‰
   - "å¤œå¸‚" -> "å¤œè§†"
   - "æµæ°”æ°”" -> "æµè§ˆå™¨"
   - "æ“¦å…¥" -> "æ’å…¥"
3. æŠ€æœ¯æœ¯è¯­ï¼šFastAPI, RAG, DeepSeek, GPU, API, Docker, Python ç­‰

ã€è¾“å‡ºæ ¼å¼ã€‘
è¿”å›žæ­£ç¡®çš„ä¸“ä¸šæœ¯è¯­åˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰ï¼š
{{
  "æ ‘èŽ“æ´¾": "æ ‘èŽ“æ´¾",
  "æ‘„åƒå¤´": "æ‘„åƒå¤´",
  "å¤œè§†": "å¤œè§†",
  "æµè§ˆå™¨": "æµè§ˆå™¨"
}}

æ³¨æ„ï¼š
- åªè¿”å›žæ­£ç¡®çš„æœ¯è¯­ï¼Œä¸è¦è¿”å›žé”™è¯¯è¯†åˆ«çš„è¯
- ä»Žæ–‡ä»¶åä¸­æå–å…³é”®çº¿ç´¢
- è¯†åˆ«å¸¸è§åŒéŸ³å­—é”™è¯¯

åªè¿”å›ž JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜Žã€‚"""
            
            # ä½¿ç”¨æ–°çº¿ç¨‹çš„äº‹ä»¶å¾ªçŽ¯è°ƒç”¨å¼‚æ­¥ generate æ–¹æ³•ï¼ˆ30ç§’è¶…æ—¶ï¼‰
            try:
                messages = [{"role": "user", "content": prompt}]
                response = run_async_in_new_loop(provider.generate(messages), timeout=30)
                logger.info(f"LLM æœ¯è¯­æå–å“åº”: {response[:200] if response else 'Empty'}...")
            except TimeoutError:
                logger.warning("âš ï¸ LLM æœ¯è¯­æå–è¶…æ—¶ï¼ˆ30ç§’ï¼‰ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
                return {}
            except Exception as e:
                logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {str(e)}")
                return {}
            
            # æå– JSON
            import json
            import re
            
            if not response:
                logger.warning("LLM è¿”å›žç©ºå“åº”")
                return {}
            
            # æ‰“å°å®Œæ•´å“åº”ç”¨äºŽè°ƒè¯•
            logger.info(f"LLM å®Œæ•´å“åº”: {response}")
            
            # å…ˆå°è¯•åŽ»é™¤ä»£ç å—æ ‡è®°ï¼ˆ```json ... ```ï¼‰
            cleaned_response = response.strip()
            if cleaned_response.startswith('```'):
                # åŽ»é™¤å¼€å¤´çš„ ```json æˆ– ```
                lines = cleaned_response.split('\n')
                if lines[0].startswith('```'):
                    lines = lines[1:]  # åŽ»é™¤ç¬¬ä¸€è¡Œ
                if lines and lines[-1].strip() == '```':
                    lines = lines[:-1]  # åŽ»é™¤æœ€åŽä¸€è¡Œ
                cleaned_response = '\n'.join(lines).strip()
            
            # å°è¯•æå– JSON å¯¹è±¡
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned_response, re.DOTALL)
            if json_match:
                try:
                    tech_terms = json.loads(json_match.group())
                    logger.info(f"è‡ªåŠ¨æå–äº† {len(tech_terms)} ä¸ªä¸“ä¸šæœ¯è¯­: {list(tech_terms.keys())[:10]}")
                    return tech_terms
                except json.JSONDecodeError as e:
                    logger.error(f"JSON è§£æžå¤±è´¥: {e}, æå–çš„æ–‡æœ¬: {json_match.group()[:200]}")
                    return {}
            else:
                logger.warning(f"LLM å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆ JSONï¼Œå“åº”å‰ 500 å­—ç¬¦: {response[:500]}")
                return {}
                
        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æžå¤±è´¥: {str(e)}")
            return {}
        except Exception as e:
            logger.error(f"æå–ä¸“ä¸šæœ¯è¯­å¤±è´¥: {str(e)}", exc_info=True)
            return {}
    
    def _correct_subtitle_with_llm(self, segments: list, **kwargs) -> list:
        """Use LLM to correct subtitle text based on context"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹ LLM æ™ºèƒ½çº é”™æµç¨‹")
        logger.info("=" * 60)
        
        llm_manager = self._get_llm_manager()
        if not llm_manager:
            logger.warning("âŒ LLM Manager ä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™çº é”™")
            return self._correct_subtitle_with_rules(segments)
        
        try:
            # åˆ†æ‰¹å¤„ç†ï¼šæ¯æ¬¡å¤„ç†50æ¡å­—å¹•ï¼ˆé¿å…promptè¿‡é•¿ï¼‰
            batch_size = 50
            total_segments = len(segments)
            logger.info(f"ðŸ“Š æ€»å…± {total_segments} æ¡å­—å¹•ï¼Œåˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ {batch_size} æ¡ï¼‰")
            
            corrected_segments = []
            for batch_start in range(0, total_segments, batch_size):
                batch_end = min(batch_start + batch_size, total_segments)
                batch = segments[batch_start:batch_end]
                
                logger.info(f"ðŸ”„ å¤„ç†ç¬¬ {batch_start+1}-{batch_end} æ¡å­—å¹•...")
                
                # âš ï¸ ç»ˆæžä¿®å¤ï¼šæ¯æ‰¹åˆ›å»ºå…¨æ–°çš„ LLMManager å®žä¾‹ï¼ˆä¸å¤ç”¨å…¨å±€å•ä¾‹ï¼‰
                # åŽŸå› ï¼šWindows ProactorEventLoop æ— æ³•åœ¨æ–°çº¿ç¨‹ä¸­å®‰å…¨æ¸…ç†æ—§äº‹ä»¶å¾ªçŽ¯çš„ httpx è¿žæŽ¥æ± 
                # è§£å†³ï¼šå®Œå…¨ç‹¬ç«‹çš„ LLMManagerï¼Œç”¨å®Œå³å¼ƒï¼Œé¿å…ä»»ä½•è¿žæŽ¥å¤ç”¨
                try:
                    from llm.llm_manager import LLMManager  # ç›´æŽ¥å¯¼å…¥ç±»ï¼Œä¸ç”¨å…¨å±€å‡½æ•°
                    batch_llm_manager = LLMManager()  # æ¯æ‰¹åˆ›å»ºæ–°å®žä¾‹
                    logger.debug("âœ… åˆ›å»ºç‹¬ç«‹ LLMManager å®žä¾‹")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ›å»º LLMManager å¤±è´¥: {e}")
                    batch_corrected = self._correct_subtitle_with_rules(batch)
                    corrected_segments.extend(batch_corrected)
                    continue
                
                # ä»Žæ–°å®žä¾‹èŽ·å– Provider
                provider = batch_llm_manager.get_provider(task_type="chinese_processing")
                if not provider:
                    provider = batch_llm_manager.get_provider()
                
                if not provider:
                    logger.warning(f"âŒ ç¬¬ {batch_start+1}-{batch_end} æ‰¹æ¬¡æ— å¯ç”¨ providerï¼Œä½¿ç”¨è§„åˆ™çº é”™")
                    batch_corrected = self._correct_subtitle_with_rules(batch)
                    corrected_segments.extend(batch_corrected)
                    continue
                
                logger.debug(f"âœ… Provider: {provider.__class__.__name__}")
                
                # Combine batch subtitle text for context
                full_text = "\n".join([f"{i+1}. {seg['text'].strip()}" for i, seg in enumerate(batch)])
                
                # èŽ·å–ä¸“ä¸šè¯æ±‡è¯å…¸ï¼Œæ•´åˆè‡ªåŠ¨æå–çš„æœ¯è¯­
                tech_terms = kwargs.get('tech_terms', {})
                
                # åŸºç¡€æŠ€æœ¯è¯æ±‡ï¼ˆä¿åº•ï¼‰
                base_terms = ["FastAPI", "DeepSeek", "R1", "Pydantic", "Ollama", "GPU", "API", 
                             "Docker", "Python", "RAG", "Transformer", "CUDA", "PyTorch", "TensorFlow",
                             "HuggingFace", "OpenAI", "GPT", "LLM", "Qwen", "Whisper", "FFmpeg"]
                
                # å¸¸è§æ‹¼éŸ³è¯¯è¯†åˆ«æ˜ å°„ï¼ˆå¸®åŠ©LLMè¯†åˆ«ï¼‰
                pinyin_map = {
                    "FastAPI": ["past api", "fast api", "æ³•æ–¯ç‰¹api", "å¸•æ–¯ç‰¹api"],
                    "RAG": ["rg", "é˜¿æ ¼", "r g"],
                    "DeepSeek": ["dpsi", "è¿ªæ™®è¥¿å…‹", "deep seek"],
                    "API": ["a p i", "åŸƒçš®çˆ±", "æŽ¥å£"],
                    "Web": ["æœªå¤‡", "å¾®åš"],
                    "Docker": ["å¤šå…‹", "é“å…‹"],
                    "GPU": ["g p u", "è®¡çš®å‹"],
                    "LLM": ["l l m", "å¤§æ¨¡åž‹"],
                    "Transformer": ["ä¼ è¾“ä½›èŽ«", "transformer"]
                }
                
                # åˆå¹¶è‡ªåŠ¨æå–çš„æœ¯è¯­ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
                if tech_terms:
                    extracted_terms = list(tech_terms.keys())
                    # åˆå¹¶åŽ»é‡
                    all_terms = list(dict.fromkeys(extracted_terms + base_terms))
                    # åªæ˜¾ç¤ºå‰15ä¸ªæœ€é‡è¦çš„æœ¯è¯­
                    key_terms = all_terms[:15]
                    logger.info(f"ðŸ”¤ ä½¿ç”¨ {len(all_terms)} ä¸ªä¸“ä¸šæœ¯è¯­ï¼Œé‡ç‚¹çº æ­£å‰15ä¸ª: {', '.join(key_terms)}")
                else:
                    key_terms = base_terms[:15]
                    logger.info(f"ðŸ”¤ ä½¿ç”¨åŸºç¡€æœ¯è¯­ {len(key_terms)} ä¸ª")
                
                # æž„å»ºæœ¯è¯­çº æ­£æç¤ºï¼ˆåŒ…å«æ‹¼éŸ³æ˜ å°„ï¼‰
                terms_hint = []
                for term in key_terms:
                    if term in pinyin_map:
                        variants = ", ".join(pinyin_map[term])
                        terms_hint.append(f"  â€¢ {term} (å¯èƒ½è¢«è¯†åˆ«ä¸º: {variants})")
                    else:
                        terms_hint.append(f"  â€¢ {term}")
                
                terms_display = "\n".join(terms_hint)
                
                # Create prompt for subtitle correction
                prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ–‡å­—å¹•çº é”™AIã€‚è¿™æ˜¯è¯­éŸ³è¯†åˆ«çš„é”™è¯¯æ–‡æœ¬ï¼ŒåŒ…å«å¤§é‡åŒéŸ³å­—é”™è¯¯ï¼Œä½ å¿…é¡»ä¸¥æ ¼çº æ­£æ‰€æœ‰é”™è¯¯ã€‚

ã€æ ¸å¿ƒä»»åŠ¡ã€‘
1ï¸âƒ£ **å…ˆç†è§£è¯­ä¹‰** - è¯»æ‡‚æ•´å¥è¯çš„æ„æ€ï¼Œå‘çŽ°ä¸é€šé¡ºçš„åœ°æ–¹
2ï¸âƒ£ **æ‰¾è¯­ä¹‰é”™è¯¯** - æ ‡è®°"æ— æ„ä¹‰"æˆ–"ä¸ç¬¦åˆé€»è¾‘"çš„è¯ç»„
3ï¸âƒ£ **ä¸Šä¸‹æ–‡çº é”™** - ç»“åˆå‰åŽæ–‡é€‰æ‹©æ­£ç¡®çš„è¯

ã€å…³é”®æœ¯è¯­å¯¹ç…§è¡¨ã€‘
{terms_display}

ã€å¿…é¡»ç»“åˆè¯­å¢ƒçº æ­£çš„é”™è¯¯ã€‘ï¼ˆå¸¦è¯­ä¹‰åˆ†æžï¼‰
1. "å–„ä»·" â†’ "ä¸Šæž¶" 
   âŒ "æœ€æ–°å–„ä»·" è¯­ä¹‰ä¸é€šï¼ˆ"å–„ä»·"ä¸æ˜¯äº§å“å‘å¸ƒçš„è¯´æ³•ï¼‰
   âœ… "æœ€æ–°ä¸Šæž¶" ç¬¦åˆè¯­å¢ƒï¼ˆå‘å¸ƒæ–°äº§å“çš„å¸¸ç”¨è¡¨è¾¾ï¼‰
   
2. "äºšä¼¯æ™ºèƒ½" â†’ "æ ‘èŽ“æ´¾"
   âŒ ä¸æ˜¯å“ç‰Œåï¼Œä½†ä¸Šä¸‹æ–‡åœ¨ä»‹ç»æ ‘èŽ“æ´¾äº§å“
   âœ… ä¸»è¯­åº”ä¸º"æ ‘èŽ“æ´¾"
   
3. "ç¼…ç§°è‰²æ½œå¤´" â†’ "æ‘„åƒå¤´"
   âŒ "ç¼…ç§°"+"è‰²æ½œå¤´" å®Œå…¨æ— æ„ä¹‰
   âœ… åº”ä¸º"æ‘„åƒå¤´"ï¼ˆæ‹¼éŸ³ shÃ¨xiÃ ngtÃ³uï¼‰
   
4. "å¤œå¸‚ç‰ˆ" â†’ "å¤œè§†ç‰ˆ"
   âŒ "å¤œå¸‚ç‰ˆæ‘„åƒå¤´" è¯­ä¹‰ä¸é€šï¼ˆå¤œå¸‚æ˜¯åœ°æ–¹ï¼‰
   âœ… "å¤œè§†ç‰ˆæ‘„åƒå¤´" ç¬¦åˆäº§å“åŠŸèƒ½æè¿°
   
5. "æ•°ç æ´¾/æ•¸ç¢¼æ´¾" â†’ "æ ‘èŽ“æ´¾"ï¼ˆå“ç‰ŒååŒéŸ³é”™è¯¯ï¼‰
6. "æµæ°”æ°”/æµå™¨æ°”" â†’ "æµè§ˆå™¨"ï¼ˆæ‹¼éŸ³é”™è¯¯ï¼‰
7. "æ“¦å…¥" â†’ "æ’å…¥"ï¼ˆåŠ¨ä½œè¯é”™è¯¯ï¼‰
8. "è®°å¿†" â†’ "ç•Œé¢"ï¼ˆéœ€çœ‹ä¸Šä¸‹æ–‡ï¼Œå¦‚"ç•Œé¢ä¸Š"ï¼‰
9. "æ€»ç»Ÿç‰ˆ" â†’ "æ ‘èŽ“æ´¾"ï¼ˆå“ç‰Œåé”™è¯¯ï¼‰

ã€çº æ­£ç¤ºä¾‹ã€‘ï¼ˆå¿…é¡»å‚è€ƒæ‰§è¡Œï¼‰
[åŽŸæ–‡] "äºšä¼¯æ™ºèƒ½æœ€æ–°å–„ä»·çš„æ•°ç æ´¾å®˜æ–¹ç¼…ç§°è‰²æ½œå¤´å¤œå¸‚ç‰ˆ"
[åˆ†æž] "äºšä¼¯æ™ºèƒ½"éžå“ç‰Œåâ†’"å–„ä»·"è¯­ä¹‰ä¸é€šâ†’"ç¼…ç§°è‰²æ½œå¤´"æ— æ„ä¹‰â†’"å¤œå¸‚ç‰ˆ"ä¸ç¬¦åˆäº§å“å±žæ€§
[çº æ­£] "æ ‘èŽ“æ´¾æœ€æ–°ä¸Šæž¶çš„æ ‘èŽ“æ´¾å®˜æ–¹æ‘„åƒå¤´å¤œè§†ç‰ˆ"

[åŽŸæ–‡] "å¤œå¸‚è‰²æ½œå¤´æ“¦å…¥åˆ°æ•°ç æ€»ç»Ÿç‰ˆ"
[åˆ†æž] "å¤œå¸‚"åº”ä¸ºåŠŸèƒ½â†’"æ“¦å…¥"åº”ä¸ºåŠ¨ä½œâ†’"æ€»ç»Ÿç‰ˆ"åº”ä¸ºå“ç‰Œ
[çº æ­£] "å¤œè§†æ‘„åƒå¤´æ’å…¥åˆ°æ ‘èŽ“æ´¾"

ã€çº æ­£åŽŸåˆ™ã€‘ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
1. **è¯­ä¹‰ä¼˜å…ˆ** - å…ˆåˆ¤æ–­å¥å­æ˜¯å¦é€šé¡ºï¼Œä¸é€šé¡ºå¿…é¡»æ”¹
2. **ä¸Šä¸‹æ–‡åˆ¤æ–­** - æ ¹æ®å‰åŽæ–‡é€‰æ‹©åˆç†çš„è¯ï¼ˆå¦‚"æœ€æ–°XX"åŽé¢å¿…é¡»æ˜¯"ä¸Šæž¶"è€Œéž"å–„ä»·"ï¼‰
3. **é€»è¾‘æ£€æŸ¥** - æ£€æŸ¥ä¸»è°“å®¾æ˜¯å¦åŒ¹é…ã€å½¢å®¹è¯æ˜¯å¦åˆç†
4. **ä¿ç•™ç»“æž„** - åªæ›¿æ¢é”™è¯¯è¯ï¼Œä¸æ”¹å˜å¥å­ç»“æž„

ã€è¾“å‡ºè¦æ±‚ã€‘
- æ¯è¡Œä¸€å¥ï¼ŒæŒ‰åºå·1ã€2ã€3...è¾“å‡º
- åªè¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•è§£é‡Š
- å¿…é¡»è¾“å‡º {len(batch)} è¡Œ

ã€åŽŸå§‹å­—å¹•ã€‘
{full_text}

ã€çº æ­£åŽã€‘
"""
                
                logger.info(f"ðŸ“¤ Batch prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
                
                # è°ƒç”¨ LLMï¼ˆä½¿ç”¨æ–°çº¿ç¨‹äº‹ä»¶å¾ªçŽ¯é¿å…å†²çªï¼Œ30ç§’è¶…æ—¶ï¼‰
                response = None
                try:
                    logger.info("ðŸ”„ è°ƒç”¨ LLM generate()...")
                    messages = [{"role": "user", "content": prompt}]
                    response = run_async_in_new_loop(provider.generate(messages), timeout=30)
                    logger.info("âœ… LLM è°ƒç”¨æˆåŠŸ")
                except TimeoutError:
                    logger.error(f"âŒ ç¬¬ {batch_start+1}-{batch_end} æ‰¹æ¬¡è°ƒç”¨è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
                    logger.warning(f"âš ï¸ å¯¹æœ¬æ‰¹æ¬¡é™çº§åˆ°è§„åˆ™çº é”™ï¼ˆä¿ç•™å‰ {len(corrected_segments)} æ¡å·²çº æ­£å­—å¹•ï¼‰")
                    batch_corrected = self._correct_subtitle_with_rules(batch)
                    corrected_segments.extend(batch_corrected)
                    continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹
                except Exception as e:
                    logger.error(f"âŒ ç¬¬ {batch_start+1}-{batch_end} æ‰¹æ¬¡è°ƒç”¨å¤±è´¥: {str(e)}")
                    logger.warning(f"âš ï¸ å¯¹æœ¬æ‰¹æ¬¡é™çº§åˆ°è§„åˆ™çº é”™ï¼ˆä¿ç•™å‰ {len(corrected_segments)} æ¡å·²çº æ­£å­—å¹•ï¼‰")
                    batch_corrected = self._correct_subtitle_with_rules(batch)
                    corrected_segments.extend(batch_corrected)
                    continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹
                
                if response:
                    # æ‰“å°å®Œæ•´å“åº”ç”¨äºŽè°ƒè¯•ï¼ˆé™åˆ¶é•¿åº¦é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                    logger.info(f"ðŸ“¥ LLM å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
                    logger.debug(f"ðŸ“¥ LLM å®Œæ•´å“åº”:\n{response[:500]}...")  # åªæ‰“å°å‰500å­—ç¬¦
                    logger.info("=" * 80)
                    
                    # Parse corrected text - åªè§£æžæœ‰åºå·çš„è¡Œ
                    corrected_lines = []
                    import re
                    for line in response.strip().split('\n'):
                        line = line.strip()
                        # ä¸¥æ ¼åŒ¹é… "æ•°å­—. æ–‡æœ¬" æ ¼å¼
                        match = re.match(r'^(\d+)\.\s*(.+)$', line)
                        if match:
                            text = match.group(2).strip()
                            if text:  # ç¡®ä¿æ–‡æœ¬éžç©º
                                corrected_lines.append(text)
                                logger.debug(f"è§£æžè¡Œ {match.group(1)}: {text[:50]}...")
                    
                    logger.info(f"âœ… æœ¬æ‰¹æ¬¡è§£æžå¾—åˆ° {len(corrected_lines)} è¡Œçº æ­£æ–‡æœ¬ï¼ŒåŽŸå§‹æœ‰ {len(batch)} æ®µ")
                    if corrected_lines:
                        logger.info(f"å‰5è¡Œç¤ºä¾‹: {corrected_lines[:5]}")
                    
                    # Update batch segments with corrected text
                    if len(corrected_lines) == len(batch):
                        for i, corrected_text in enumerate(corrected_lines):
                            batch[i]['text'] = corrected_text
                        corrected_segments.extend(batch)
                        logger.info(f"âœ… æ‰¹æ¬¡çº æ­£æˆåŠŸï¼å·²å¤„ç† {len(corrected_segments)}/{total_segments} æ¡å­—å¹•")
                    else:
                        # è¡Œæ•°ä¸åŒ¹é…ï¼šå¯¹æœ¬æ‰¹æ¬¡ä½¿ç”¨è§„åˆ™çº é”™ï¼Œä½†ä¿ç•™å…¶ä»–æ‰¹æ¬¡çš„LLMçº é”™ç»“æžœ
                        logger.warning(f"âš ï¸ æœ¬æ‰¹æ¬¡çº æ­£è¡Œæ•° ({len(corrected_lines)}) ä¸ŽåŽŸå§‹æ®µæ•° ({len(batch)}) ä¸åŒ¹é…")
                        logger.warning(f"âš ï¸ å¯¹æœ¬æ‰¹æ¬¡ä½¿ç”¨è§„åˆ™çº é”™ï¼ˆä¿ç•™å‰ {len(corrected_segments)} æ¡å·²çº æ­£å­—å¹•ï¼‰")
                        # å¯¹å½“å‰æ‰¹æ¬¡åº”ç”¨è§„åˆ™çº é”™
                        batch_corrected = self._correct_subtitle_with_rules(batch)
                        corrected_segments.extend(batch_corrected)
                        logger.info(f"âœ… æ‰¹æ¬¡é™çº§å®Œæˆï¼å·²å¤„ç† {len(corrected_segments)}/{total_segments} æ¡å­—å¹•")
                else:
                    logger.warning(f"âŒ ç¬¬ {batch_start+1}-{batch_end} æ‰¹æ¬¡å“åº”ä¸ºç©º")
                    logger.warning(f"âš ï¸ å¯¹æœ¬æ‰¹æ¬¡é™çº§åˆ°è§„åˆ™çº é”™ï¼ˆä¿ç•™å‰ {len(corrected_segments)} æ¡å·²çº æ­£å­—å¹•ï¼‰")
                    batch_corrected = self._correct_subtitle_with_rules(batch)
                    corrected_segments.extend(batch_corrected)
            
            # æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆ
            logger.info(f"âœ…âœ…âœ… LLM æ™ºèƒ½çº é”™å…¨éƒ¨å®Œæˆï¼å…±çº æ­£ {len(corrected_segments)} æ¡å­—å¹•")
            logger.info("=" * 60)
            return corrected_segments
            
        except Exception as e:
            logger.error(f"Failed to correct subtitles with LLM: {e}, falling back to rule-based correction")
            return self._correct_subtitle_with_rules(segments)

    
    def _correct_subtitle_with_rules(self, segments: list) -> list:
        """Use rule-based correction for common errors"""
        # Common homophone errors in Chinese speech recognition
        correction_rules = {
            # Technology terms - First pass (specific phrases)
            'äºšä¼¯æ™ºèƒ½æœ€æ–°å–„ä»·çš„æ•°ç æ´¾å®˜æ–¹ç¼…ç§°è‰²æ½œå¤´å¤œå¸‚ç‰ˆ': 'æ ‘èŽ“æ´¾æœ€æ–°ä¸Šæž¶çš„æ ‘èŽ“æ´¾å®˜æ–¹æ‘„åƒå¤´å¤œè§†ç‰ˆ',
            'æ•°ç æ´¾å®˜æ–¹ç¼…ç§°è‰²æ½œå¤´': 'æ ‘èŽ“æ´¾å®˜æ–¹æ‘„åƒå¤´',
            'å®˜æ–¹ç¼…ç§°è‰²æ½œå¤´': 'å®˜æ–¹æ‘„åƒå¤´',
            'æ•°ç æ€»ç»Ÿç‰ˆçš„è‰²æ½œå¤´æ–¹å‘æ“æŽ§': 'æ ‘èŽ“æ´¾çš„æ‘„åƒå¤´æŽ¥å£',
            'è‰²æ½œå¤´æ–¹å‘æ“æŽ§': 'æ‘„åƒå¤´æŽ¥å£',
            'æ•°ç æ€»ç»Ÿç‰ˆ': 'æ ‘èŽ“æ´¾',
            'æ•°ç ç³»ç»Ÿ': 'æ ‘èŽ“æ´¾ç³»ç»Ÿ',
            'æœ€æ–°å–„ä»·': 'æœ€æ–°ä¸Šæž¶',
            'å–„ä»·çš„': 'ä¸Šæž¶çš„',
            'è‰²æ½œå¤´': 'æ‘„åƒå¤´',
            'å¤œå¸‚ç‰ˆ': 'å¤œè§†ç‰ˆ',
            'è‰²äº§ä¸šçš„': 'æ‘„åƒçš„',
            'è‰²äº§ä¸š': 'æ‘„åƒ',
            'æµæ°”æ°”': 'æµè§ˆå™¨',
            'æµå™¨æ°”': 'æµè§ˆå™¨',
            'æ•°ç æ´¾': 'æ ‘èŽ“æ´¾',
            'è£…ç”¨è‰²æ½œå¤´': 'ä¸“ç”¨æ‘„åƒå¤´',
            'è£…ç”¨': 'ä¸“ç”¨',
            'äºšä¼¯æ™ºèƒ½': 'æ ‘èŽ“æ´¾',
            'è§†é¢‘IPåœ°å€': 'IPåœ°å€',
            
            # Second pass (individual words)
            'å–„ä»·': 'ä¸Šæž¶',
            'ç¼…ç§°': 'æ‘„åƒ',
            'æ½œå¤´': 'åƒå¤´',
            'æ“¦å…¥': 'æ’å…¥',
            'å¤œå¸‚': 'å¤œè§†',
            'æ€»ç»Ÿç‰ˆ': 'æ´¾',
            'æ€»ç»Ÿ': 'æ´¾',
            'è®°å¿†ä¸Š': 'ç•Œé¢ä¸Š',
            'è®°å¿†': 'ç•Œé¢',
            'è§†é¢‘IP': 'IP',
            
            # Common misrecognitions
            'æ°”æ°”': 'å™¨',
            'æ°”': 'å™¨',
            'ç”£': 'äº§',
        }
        
        logger.info("Applying rule-based subtitle correction...")
        corrected_count = 0
        
        for segment in segments:
            original_text = segment['text']
            corrected_text = original_text
            
            # Apply correction rules (ordered by length to handle phrases first)
            sorted_rules = sorted(correction_rules.items(), key=lambda x: len(x[0]), reverse=True)
            for wrong, correct in sorted_rules:
                if wrong in corrected_text:
                    before = corrected_text
                    corrected_text = corrected_text.replace(wrong, correct)
                    if before != corrected_text:
                        corrected_count += 1
                        logger.debug(f"Applied rule: '{wrong}' -> '{correct}'")
            
            # Update segment if changed
            if corrected_text != original_text:
                segment['text'] = corrected_text
                logger.debug(f"Corrected: '{original_text}' -> '{corrected_text}'")
        
        logger.info(f"Rule-based correction applied {corrected_count} fixes to {len(segments)} segments")
        return segments
    
    def _extract_audio(self, video_path: str, audio_path: str) -> bool:
        """Extract audio from video"""
        try:
            cmd = [
                "-i", video_path,
                "-vn",  # No video
                "-acodec", "pcm_s16le",  # PCM audio
                "-ar", "16000",  # 16kHz sample rate for Whisper
                "-ac", "1",  # Mono
                audio_path,
                "-y"
            ]
            result = self._run_ffmpeg(cmd)
            return result.get("success", False)
        except Exception as e:
            logger.error(f"Failed to extract audio: {e}")
            return False
    
    def _format_timestamp(self, seconds: float) -> str:
        """Format seconds to SRT timestamp format"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds - int(seconds)) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"
    
    def _convert_to_simplified_chinese(self, text: str) -> str:
        """Convert traditional Chinese to simplified Chinese"""
        try:
            from opencc import OpenCC
            cc = OpenCC('t2s')  # Traditional to Simplified
            return cc.convert(text)
        except ImportError:
            # If OpenCC is not available, try using a simple mapping
            logger.warning("OpenCC not installed, using basic conversion")
            # Basic traditional to simplified mapping
            trans_map = {
                'æ­¡è¿Ž': 'æ¬¢è¿Ž', 'è§€çœ‹': 'è§‚çœ‹', 'æ•¸ç¢¼': 'æ•°ç ', 'ç·¬ç¨±': 'ç¼…ç§°',
                'æ“¦å…¥': 'æ’å…¥', 'ç¸½çµ±': 'æ€»ç»Ÿ', 'ç¹¼çºŒ': 'ç»§ç»­', 'è®“': 'è®©',
                'é‹è¡Œ': 'è¿è¡Œ', 'æ°£': 'å™¨', 'è¨˜æ†¶': 'ç•Œé¢', 'ç”¢æ¥­': 'æ‘„åƒ',
                'çµæŸ': 'ç»“æŸ', 'è¬è¬': 'è°¢è°¢', 'åƒ¹': 'ä»·', 'è¦–é »': 'è§†é¢‘',
                'æ“¦': 'æ’', 'ç¶«': 'çº¿', 'æ½›é ­': 'æ‘„åƒå¤´', 'å¤œå¸‚': 'å¤œè§†'
            }
            for trad, simp in trans_map.items():
                text = text.replace(trad, simp)
            return text
        except Exception as e:
            logger.warning(f"Conversion failed: {e}, using original text")
            return text
    
    def _translate_to_english(self, text: str) -> str:
        """Translate Chinese text to English using LLM"""
        llm_manager = self._get_llm_manager()
        if not llm_manager:
            logger.warning("LLM not available for translation")
            # Return a placeholder that indicates translation is needed
            return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
        
        try:
            # Get provider by task type
            provider = llm_manager.get_provider(task_type="subtitle_translation")
            if not provider:
                logger.warning("No translation provider available")
                # Return Chinese text as fallback
                return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
            
            prompt = f"""Translate the following Chinese subtitle to natural English. Keep it concise and suitable for video subtitles.
Only output the English translation, no explanations.

Chinese: {text}
English:"""
            
            try:
                messages = [{"role": "user", "content": prompt}]
                response = run_async_in_new_loop(provider.generate(messages), timeout=30)
                if response:
                    return response.strip()
                else:
                    return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
            except TimeoutError:
                logger.warning(f"Translation timeout (30s) for: {text[:30]}...")
                return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
            except Exception as e:
                logger.warning(f"Translation API call failed: {e}")
                return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
        except Exception as e:
            logger.error(f"Translation failed: {e}")
            return f"[EN: {text[:30]}...]" if len(text) > 30 else f"[EN: {text}]"
    
    def _write_srt(self, segments: list, output_path: str, convert_to_simplified: bool = True, bilingual: bool = False) -> bool:
        """Write segments to SRT file
        
        Args:
            segments: List of subtitle segments
            output_path: Output SRT file path
            convert_to_simplified: Convert traditional Chinese to simplified
            bilingual: Generate bilingual (Chinese + English) subtitles
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                for i, segment in enumerate(segments, 1):
                    start = self._format_timestamp(segment['start'])
                    end = self._format_timestamp(segment['end'])
                    text = segment['text'].strip()
                    
                    # Convert to simplified Chinese if requested
                    if convert_to_simplified:
                        text = self._convert_to_simplified_chinese(text)
                    
                    f.write(f"{i}\n")
                    f.write(f"{start} --> {end}\n")
                    
                    if bilingual:
                        # Write Chinese and English on separate lines
                        english_text = segment.get('english_text', '')
                        if not english_text:
                            # Translate if not already translated
                            english_text = self._translate_to_english(text)
                            segment['english_text'] = english_text
                        
                        f.write(f"{text}\n")
                        f.write(f"{english_text}\n\n")
                    else:
                        f.write(f"{text}\n\n")
            return True
        except Exception as e:
            logger.error(f"Failed to write SRT file: {e}")
            return False
    
    def execute(
        self,
        video_path: str,
        language: str = "zh",
        output_path: Optional[str] = None,
        model: str = "base",
        embed_subtitle: bool = False,
        bilingual: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """Generate subtitle file using Whisper and optionally embed it into video
        
        Args:
            video_path: Path to input video file
            language: Language code for speech recognition (default: zh)
            output_path: Path to output file
            model: Whisper model to use (default: base)
            embed_subtitle: Whether to burn subtitle into video
            bilingual: Generate bilingual Chinese+English subtitles
            **kwargs: Additional arguments (use_llm_correction, etc.)
        """
        if not validate_video_file(video_path):
            return {"success": False, "error": "Invalid video file"}
        
        # Determine subtitle file path
        if output_path is None:
            if embed_subtitle:
                # For embedded subtitle, output is a video file
                base = str(Path(video_path).stem)
                output_path = str(Path(video_path).parent / f"{base}-subtitle.avi")
            else:
                # For standalone subtitle, output is SRT file
                output_path = str(Path(video_path).with_suffix('.srt'))
        else:
            output_path = str(ensure_output_dir(output_path))
        
        # Generate SRT file path
        if embed_subtitle:
            # åŸºäºŽè¾“å‡ºæ–‡ä»¶è·¯å¾„ç”Ÿæˆ SRTï¼Œé¿å…è¦†ç›–æºæ–‡ä»¶çš„ SRT
            srt_path = str(Path(output_path).with_suffix('.srt'))
        else:
            # ä¸åµŒå…¥æ—¶ï¼Œå¼ºåˆ¶ä½¿ç”¨.srtåŽç¼€ï¼ˆå³ä½¿ä¼ å…¥çš„æ˜¯.mp4ï¼‰
            srt_path = str(Path(output_path).with_suffix('.srt'))
        
        try:
            # ç¡®ä¿ ffmpeg åœ¨ PATH ä¸­ï¼ˆWhisper éœ€è¦è°ƒç”¨ ffmpegï¼‰
            ffmpeg_dir = str(Path(self.ffmpeg_path).parent)
            if ffmpeg_dir not in os.environ.get('PATH', ''):
                os.environ['PATH'] = ffmpeg_dir + os.pathsep + os.environ.get('PATH', '')
                logger.info(f"Added ffmpeg directory to PATH: {ffmpeg_dir}")
            
            # Step 1: Extract audio from video
            logger.info(f"Extracting audio from {video_path}")
            audio_path = str(Path(video_path).with_suffix('.wav'))
            if not self._extract_audio(video_path, audio_path):
                return {"success": False, "error": "Failed to extract audio"}
            
            # Step 2: Transcribe audio using Whisper
            logger.info(f"Transcribing audio with Whisper (language: {language})")
            model_obj = self._get_whisper_model(model)
            result = model_obj.transcribe(audio_path, language=language)
            
            # Step 2.5: Auto-extract technical terms if not provided
            segments = result['segments']
            tech_terms = kwargs.get('tech_terms', {})
            
            if not tech_terms and kwargs.get('auto_extract_terms', True):
                logger.info("è‡ªåŠ¨æå–ä¸“ä¸šæœ¯è¯­...")
                # ä»Žåˆæ­¥å­—å¹•ä¸­æå–ä¸“ä¸šæœ¯è¯­
                initial_text = "\n".join([seg['text'] for seg in segments])
                extracted_terms = self._extract_technical_terms(video_path, initial_text)
                if extracted_terms:
                    tech_terms = extracted_terms
                    kwargs['tech_terms'] = tech_terms
                    logger.info(f"è‡ªåŠ¨æå–äº† {len(tech_terms)} ä¸ªä¸“ä¸šæœ¯è¯­")
            
            # First convert to simplified Chinese
            for seg in segments:
                seg['text'] = self._convert_to_simplified_chinese(seg['text'])
            
            # Then apply corrections
            correction_count = 0
            
            # âš ï¸ é»˜è®¤ä½¿ç”¨è§„åˆ™çº é”™ï¼ˆé¿å… Windows äº‹ä»¶å¾ªçŽ¯å†²çªï¼‰
            # å¦‚æžœéœ€è¦ LLM çº é”™ï¼Œè¯·åœ¨ä¸»å¼‚æ­¥çŽ¯å¢ƒä¸­è°ƒç”¨
            use_rule_correction = not kwargs.get('use_llm_correction', False)
            
            if use_rule_correction:
                # ç›´æŽ¥ä½¿ç”¨è§„åˆ™çº é”™ï¼ˆç¨³å®šã€å¿«é€Ÿã€æ•ˆæžœå¥½ï¼‰
                logger.info("Applying rule-based subtitle correction...")
                segments = self._correct_subtitle_with_rules(segments)
            else:
                # å°è¯• LLM çº é”™ï¼ˆå¯èƒ½å› äº‹ä»¶å¾ªçŽ¯é—®é¢˜å¤±è´¥ï¼‰
                logger.info("Attempting LLM-based subtitle correction...")
                try:
                    segments = self._correct_subtitle_with_llm(segments, **kwargs)
                    correction_count = len(segments)
                    logger.info("âœ… LLM çº é”™æˆåŠŸ")
                except RuntimeError as e:
                    if 'Event loop is closed' in str(e):
                        logger.warning("âš ï¸ LLM è°ƒç”¨å¤±è´¥ï¼ˆäº‹ä»¶å¾ªçŽ¯å†²çªï¼‰ï¼Œé™çº§åˆ°è§„åˆ™çº é”™")
                        segments = self._correct_subtitle_with_rules(segments)
                    else:
                        raise
                except Exception as e:
                    logger.warning(f"âš ï¸ LLM çº é”™å¼‚å¸¸: {e}ï¼Œé™çº§åˆ°è§„åˆ™çº é”™")
                    segments = self._correct_subtitle_with_rules(segments)
            
            # Step 2.6: Translate to English if bilingual mode
            if bilingual:
                logger.info("Translating subtitles to English for bilingual mode...")
                for seg in segments:
                    if 'english_text' not in seg:
                        seg['english_text'] = self._translate_to_english(seg['text'])
            
            # Step 3: Write SRT file (skip conversion since already done)
            logger.info(f"Writing subtitle to {srt_path}")
            if not self._write_srt(segments, srt_path, convert_to_simplified=False, bilingual=bilingual):
                return {"success": False, "error": "Failed to write SRT file"}
            
            # Clean up audio file
            if os.path.exists(audio_path):
                os.remove(audio_path)
            
            # Step 4: Embed subtitle if requested (ä½¿ç”¨è½¯å­—å¹•æµï¼Œä¸çƒ§å½•åˆ°ç”»é¢)
            if embed_subtitle:
                logger.info(f"Embedding subtitle stream (soft subtitle) into video: {video_path}")
                
                # Convert to absolute paths
                abs_video_path = str(Path(video_path).absolute())
                abs_srt_path = str(Path(srt_path).absolute())
                
                # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
                temp_output = str(Path(video_path).parent / f"{Path(video_path).stem}_temp{Path(video_path).suffix}")
                abs_temp_output = str(Path(temp_output).absolute())
                
                # ä½¿ç”¨è½¯å­—å¹•ï¼šå°†å­—å¹•æµåµŒå…¥è§†é¢‘å®¹å™¨ï¼Œä¸é‡æ–°ç¼–ç è§†é¢‘
                logger.info(f"Adding subtitle stream using: {abs_srt_path}")
                cmd = [
                    "-i", abs_video_path,
                    "-i", abs_srt_path,
                    "-c:v", "copy",  # è§†é¢‘æµç›´æŽ¥å¤åˆ¶ï¼Œä¸é‡æ–°ç¼–ç 
                    "-c:a", "copy",  # éŸ³é¢‘æµç›´æŽ¥å¤åˆ¶
                    "-c:s", "mov_text",  # å­—å¹•ç¼–ç æ ¼å¼ï¼ˆMP4ç”¨mov_textï¼‰
                    "-metadata:s:s:0", "language=chi",  # è®¾ç½®å­—å¹•è¯­è¨€
                    "-metadata:s:s:0", "title=Chinese",  # è®¾ç½®å­—å¹•æ ‡é¢˜
                    abs_temp_output,
                    "-y"
                ]
                
                embed_result = self._run_ffmpeg(cmd)
                
                if not embed_result.get("success", False):
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(temp_output):
                        os.remove(temp_output)
                    return {
                        "success": False,
                        "error": f"Failed to embed subtitle: {embed_result.get('error', 'Unknown error')}",
                        "srt_path": srt_path
                    }
                
                # æˆåŠŸåŽï¼ŒåŽŸåœ°æ›¿æ¢æºæ–‡ä»¶
                try:
                    # åˆ é™¤æºæ–‡ä»¶
                    os.remove(video_path)
                    # é‡å‘½åä¸´æ—¶æ–‡ä»¶ä¸ºæºæ–‡ä»¶å
                    os.rename(temp_output, video_path)
                    logger.info(f"Successfully replaced source file with subtitled version: {video_path}")
                except Exception as e:
                    return {
                        "success": False,
                        "error": f"Failed to replace source file: {str(e)}",
                        "srt_path": srt_path
                    }
                
                return {
                    "success": True,
                    "output_path": video_path,  # è¿”å›žæºæ–‡ä»¶è·¯å¾„ï¼ˆå·²è¢«æ›¿æ¢ï¼‰
                    "srt_path": srt_path,
                    "extracted_terms": tech_terms,
                    "correction_count": correction_count,
                    "message": f"Subtitle embedded into source file successfully (soft subtitle, original encoding preserved)"
                }
            else:
                return {
                    "success": True,
                    "output_path": srt_path,
                    "srt_path": srt_path,
                    "extracted_terms": tech_terms,
                    "correction_count": correction_count,
                    "message": f"Subtitle generated successfully"
                }
            
        except Exception as e:
            logger.error(f"Subtitle generation failed: {e}", exc_info=True)
            # Clean up temporary files
            for tmp_file in [audio_path, srt_path if embed_subtitle else None]:
                if tmp_file and os.path.exists(tmp_file):
                    try:
                        os.remove(tmp_file)
                    except:
                        pass
            return {"success": False, "error": str(e)}


class FormatTool(MediaTool):
    """Tool for format conversion"""
    
    def execute(
        self,
        input_path: str,
        output_format: str,
        output_path: str,
        resolution: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Convert video format"""
        if not validate_video_file(input_path):
            return {"success": False, "error": "Invalid video file"}
        
        output_path = str(ensure_output_dir(output_path))
        
        cmd = ["-i", input_path]
        
        # Add resolution filter if specified
        if resolution:
            if resolution == "1080p":
                cmd.extend(["-vf", "scale=1920:1080"])
            elif resolution == "720p":
                cmd.extend(["-vf", "scale=1280:720"])
            elif resolution == "480p":
                cmd.extend(["-vf", "scale=854:480"])
        
        # Add output format and path
        cmd.extend(["-c:v", "libx264", "-c:a", "aac", output_path, "-y"])
        
        return self._run_ffmpeg(cmd)


class ImageTool:
    """Tool for image processing with smart rotation based on person detection"""
    
    def __init__(self):
        if not PIL_AVAILABLE:
            raise ImportError("Pillow is not installed. Install it with: pip install Pillow")
        
        # Initialize YOLO model for person detection (lazy loading)
        self._yolo_model = None
        self._yolo_available = YOLO_AVAILABLE
    
    def _get_yolo_model(self):
        """Get or initialize YOLO model for person detection"""
        if not self._yolo_available:
            return None
        
        if self._yolo_model is None:
            try:
                # Use YOLOv8n (nano) for faster inference, can detect person class (class 0)
                self._yolo_model = YOLO('yolov8n.pt')
                logger.info("YOLO model loaded successfully")
            except Exception as e:
                logger.warning(f"Failed to load YOLO model: {e}. Smart rotation will be disabled.")
                self._yolo_available = False
                return None
        
        return self._yolo_model
    
    def _detect_person_and_analyze_orientation(self, img: Image.Image) -> Dict[str, Any]:
        """Detect person in image and analyze orientation"""
        if not self._yolo_available:
            return {"detected": False, "rotation_needed": 0}
        
        model = self._get_yolo_model()
        if model is None:
            return {"detected": False, "rotation_needed": 0}
        
        try:
            # Convert PIL to numpy array for YOLO
            img_array = np.array(img)
            
            # Run detection (person class is 0 in COCO dataset)
            results = model(img_array, classes=[0], verbose=False)  # Only detect person class
            
            if len(results) == 0 or len(results[0].boxes) == 0:
                return {"detected": False, "rotation_needed": 0}
            
            # Get the largest person bounding box (most prominent person)
            boxes = results[0].boxes
            largest_box = None
            largest_area = 0
            
            img_width, img_height = img.size
            
            for box in boxes:
                # Get box coordinates
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                area = (x2 - x1) * (y2 - y1)
                
                if area > largest_area:
                    largest_area = area
                    largest_box = {
                        "x1": float(x1),
                        "y1": float(y1),
                        "x2": float(x2),
                        "y2": float(y2),
                        "width": float(x2 - x1),
                        "height": float(y2 - y1),
                        "center_x": float((x1 + x2) / 2),
                        "center_y": float((y1 + y2) / 2),
                    }
            
            if largest_box is None:
                return {"detected": False, "rotation_needed": 0}
            
            # Analyze person orientation
            person_width = largest_box["width"]
            person_height = largest_box["height"]
            person_aspect_ratio = person_width / person_height if person_height > 0 else 1.0
            
            # Analyze image orientation
            image_aspect_ratio = img_width / img_height if img_height > 0 else 1.0
            
            # Determine if rotation is needed
            rotation_needed = 0
            
            # If image is landscape (wide) and person is also wide (horizontal)
            # Or if person's bounding box is wider than tall, person might be horizontal
            if image_aspect_ratio > 1.0:  # Landscape image
                if person_aspect_ratio > 1.2:  # Person is wider than tall (horizontal)
                    # Person is horizontal in landscape image, rotate 90 degrees CCW
                    rotation_needed = 90
                elif person_aspect_ratio < 0.8:  # Person is taller than wide (vertical)
                    # Person is already vertical, might need -90 rotation
                    rotation_needed = -90
            else:  # Portrait image
                if person_aspect_ratio > 1.2:  # Person is wider than tall
                    # Person is horizontal in portrait, rotate 90 degrees
                    rotation_needed = 90
            
            logger.info(f"Person detected: aspect_ratio={person_aspect_ratio:.2f}, rotation_needed={rotation_needed}")
            
            return {
                "detected": True,
                "rotation_needed": rotation_needed,
                "person_box": largest_box,
                "person_aspect_ratio": person_aspect_ratio,
                "image_aspect_ratio": image_aspect_ratio
            }
        except Exception as e:
            logger.warning(f"YOLO detection failed: {e}")
            return {"detected": False, "rotation_needed": 0}
    
    def execute(
        self,
        input_path: str,
        output_path: str,
        width: Optional[int] = None,
        height: Optional[int] = None,
        aspect_ratio: Optional[str] = None,  # e.g., "9:16" for vertical
        resize_mode: str = "fit",  # "fit"(pad), "cover"(fill+crop), "stretch"
        smart_rotate: bool = True,  # Enable smart rotation based on person detection
        **kwargs
    ) -> Dict[str, Any]:
        """Process image: resize, convert format, etc."""
        try:
            if not os.path.exists(input_path):
                return {"success": False, "error": f"Input file not found: {input_path}"}
            
            # Open image
            img = Image.open(input_path)
            original_width, original_height = img.size
            
            # Smart rotation: detect person and rotate if needed
            rotation_applied = 0
            if smart_rotate and (aspect_ratio in ("9:16", "vertical") or (width and height and height > width)):
                detection_result = self._detect_person_and_analyze_orientation(img)
                if detection_result.get("detected") and detection_result.get("rotation_needed") != 0:
                    rotation_needed = detection_result["rotation_needed"]
                    # Rotate image
                    if rotation_needed == 90:
                        img = img.rotate(-90, expand=True)  # Rotate counter-clockwise
                        rotation_applied = 90
                    elif rotation_needed == -90:
                        img = img.rotate(90, expand=True)  # Rotate clockwise
                        rotation_applied = -90
                    # Update dimensions after rotation
                    original_width, original_height = img.size
                    logger.info(f"Applied rotation: {rotation_applied} degrees")
            
            # Determine target dimensions
            target_width = width
            target_height = height
            
            # If aspect ratio is specified, calculate dimensions
            if aspect_ratio and not (width and height):
                if aspect_ratio == "9:16" or aspect_ratio == "vertical":
                    # Common phone vertical aspect ratio
                    if width:
                        target_height = int(width * 16 / 9)
                    elif height:
                        target_width = int(height * 9 / 16)
                    else:
                        # Use common phone vertical size: 1080x1920
                        target_width = 1080
                        target_height = 1920
                elif ":" in aspect_ratio:
                    ratio_parts = aspect_ratio.split(":")
                    ratio_w = float(ratio_parts[0])
                    ratio_h = float(ratio_parts[1])
                    if width:
                        target_height = int(width * ratio_h / ratio_w)
                    elif height:
                        target_width = int(height * ratio_w / ratio_h)
                    else:
                        # Default to 1080 width
                        target_width = 1080
                        target_height = int(1080 * ratio_h / ratio_w)
            
            # If no dimensions specified, use defaults
            if not target_width and not target_height:
                target_width = 1080
                target_height = 1920
            
            # Ensure output directory exists
            output_path = str(ensure_output_dir(output_path))
            
            # Normalize and validate resize_mode
            resize_mode = (resize_mode or "fit").lower().strip()
            if resize_mode == "crop":
                # Backward-compat alias: crop behaves like cover (fill+crop)
                resize_mode = "cover"

            # Resize image based on mode
            if resize_mode == "fit":
                # Fit image maintaining aspect ratio, add padding if needed (letterbox)
                img.thumbnail((target_width, target_height), Image.Resampling.LANCZOS)
                new_img = Image.new("RGB", (target_width, target_height), (255, 255, 255))
                paste_x = (target_width - img.size[0]) // 2
                paste_y = (target_height - img.size[1]) // 2
                new_img.paste(img, (paste_x, paste_y))
                img = new_img
            elif resize_mode == "cover":
                # Cover target size while preserving aspect ratio, then center-crop (no blank bars)
                ow, oh = img.size
                scale = max(target_width / ow, target_height / oh)
                new_w = max(1, int(round(ow * scale)))
                new_h = max(1, int(round(oh * scale)))
                img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)

                left = max(0, (new_w - target_width) // 2)
                top = max(0, (new_h - target_height) // 2)
                right = left + target_width
                bottom = top + target_height
                img = img.crop((left, top, right, bottom))
            else:  # stretch
                # Stretch to exact dimensions (may distort)
                img = img.resize((target_width, target_height), Image.Resampling.LANCZOS)
            
            # Convert to RGB if necessary (for JPEG compatibility)
            if img.mode != "RGB" and output_path.lower().endswith(('.jpg', '.jpeg')):
                img = img.convert("RGB")
            
            # Save image
            img.save(output_path, quality=95)
            
            result = {
                "success": True,
                "output_path": output_path,
                "original_size": f"{original_width}x{original_height}",
                "new_size": f"{target_width}x{target_height}"
            }
            
            if rotation_applied != 0:
                result["rotation_applied"] = rotation_applied
                result["smart_rotation"] = True
            
            return result
        except Exception as e:
            return {"success": False, "error": str(e)}


class OptimizeTool(MediaTool):
    """Tool for audio/video optimization"""
    
    def execute(
        self,
        input_path: str,
        output_path: str,
        optimize_type: str = "audio",  # "audio" or "video"
        **kwargs
    ) -> Dict[str, Any]:
        """Optimize audio or video"""
        if not validate_video_file(input_path):
            return {"success": False, "error": "Invalid video file"}
        
        output_path = str(ensure_output_dir(output_path))
        
        cmd = ["-i", input_path]
        
        if optimize_type == "audio":
            # Audio denoising and normalization
            cmd.extend([
                "-af", "highpass=f=200,lowpass=f=3000,volume=1.5",
                "-c:v", "copy",  # Copy video stream
                output_path,
                "-y"
            ])
        elif optimize_type == "video":
            # Video enhancement (basic)
            cmd.extend([
                "-vf", "eq=contrast=1.2:brightness=0.05:saturation=1.1",
                "-c:a", "copy",  # Copy audio stream
                output_path,
                "-y"
            ])
        else:
            return {"success": False, "error": f"Unknown optimize_type: {optimize_type}"}
        
        return self._run_ffmpeg(cmd)
